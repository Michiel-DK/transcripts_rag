{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import BaseNode, TransformComponent\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.legacy.retrievers.bm25_retriever import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings.llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "Settings.llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "Settings.embedding = OllamaEmbedding(model_name=\"nomic-embed-text:v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMENSION=512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FaisVectorStore to store embeddings\n",
    "fais_index = faiss.IndexFlatL2(EMBED_DIMENSION)\n",
    "vector_store = FaissVectorStore(faiss_index=fais_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    Transformation to be used within the ingestion pipeline.\n",
    "    Cleans clutters from texts.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs) -> List[BaseNode]:\n",
    "        \n",
    "        for node in nodes:\n",
    "            node.text = node.text.replace('\\t', ' ') # Replace tabs with spaces\n",
    "            node.text = node.text.replace(' \\n', ' ') # Replace paragprah seperator with spacaes\n",
    "            \n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline instantiation with: \n",
    "# node parser, custom transformer, vector store and documents\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        TextCleaner()\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    "    )\n",
    "\n",
    "# Run the pipeline to get nodes\n",
    "lyft_docs = SimpleDirectoryReader(\n",
    "        input_files=[\"../data/10k/lyft_2021.pdf\"]\n",
    "    ).load_data()\n",
    "uber_docs = SimpleDirectoryReader(\n",
    "        input_files=[\"../data/10k/uber_2021.pdf\"]\n",
    "    ).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lyft = pipeline.run(documents=lyft_docs)\n",
    "nodes_uber = pipeline.run(documents=uber_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lyft_index = VectorStoreIndex(nodes= nodes_lyft, embed_model=Settings.embedding)\n",
    "lyft_index.storage_context.persist(persist_dir=\"../storage/lyft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_index = VectorStoreIndex(nodes= nodes_uber, embed_model=Settings.embedding)\n",
    "uber_index.storage_context.persist(persist_dir=\"../storage/uber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\n",
    "uber_engine = uber_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=lyft_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lyft_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Lyft financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool. \"\n",
    "                \"The input is used to power a semantic search engine.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=uber_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"uber_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Uber financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool. \"\n",
    "                \"The input is used to power a semantic search engine.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# needs llama_index-packs-agents-coa\n",
    "from llama_index.packs.agents_coa import CoAAgentPack\n",
    "\n",
    "pack = CoAAgentPack(tools=query_engine_tools, llm=Settings.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Available Parsed Functions ====\n",
      "def lyft_10k(input: string):\n",
      "   \"\"\"Provides information about Lyft financials for year 2021. Use a detailed plain text question as input to the tool. The input is used to power a semantic search engine.\"\"\"\n",
      "    ...\n",
      "def uber_10k(input: string):\n",
      "   \"\"\"Provides information about Uber financials for year 2021. Use a detailed plain text question as input to the tool. The input is used to power a semantic search engine.\"\"\"\n",
      "    ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/llms/langchain/base.py:95: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  lc_message = self._llm.predict_messages(messages=lc_messages, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generated Chain of Abstraction ====\n",
      "Abstract plan of reasoning:\n",
      "To answer the question, we need to compare the revenue growth of Uber and Lyft in 2021. We will first get the revenue information for Lyft in 2021 by asking [FUNC lyft_10k(\"What was Lyft's revenue in 2021 and how did it change from 2020 to 2021?\") = y1]. Then, we will get the revenue information for Uber in 2021 by asking [FUNC uber_10k(\"What was Uber's revenue in 2021 and how did it change from 2020 to 2021?\") = y2]. Finally, we will compare the revenue growth of Uber and Lyft in 2021 by calculating the difference between y2 and y1.\n",
      "==== Executing lyft_10k with inputs [\"What was Lyft's revenue in 2021 and how did it change from 2020 to 2021?\"] ====\n",
      "==== Executing uber_10k with inputs [\"What was Uber's revenue in 2021 and how did it change from 2020 to 2021?\"] ====\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "response = pack.run(\"How did Ubers revenue growth compare to Lyfts in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the previous reasoning, we can conclude that Lyft's revenue growth in 2021 was 36% compared to 2020. However, due to the lack of explicit information about Uber's total revenue in 2021, we cannot accurately compare the revenue growth of Uber and Lyft in 2021. We can only infer that Uber's revenue in 2021 was lower than in 2020, with a significant decrease in All Other revenue, but the exact amount of the decrease is not specified.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcripts_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
