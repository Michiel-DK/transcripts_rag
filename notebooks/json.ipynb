{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('../json_data/transcripts_2024.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_ls = []\n",
    "\n",
    "for sub_ls in data:\n",
    "    for i in sub_ls:\n",
    "        new_data_ls.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6042"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(new_data_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_ls = ['AAPL', 'AMZN', 'GOOGL', 'MSFT', 'TSLA', 'CRSP', 'ABNB', 'ACN', 'ADYEY', 'ASML', 'CRM', 'EUXTF', 'FSLR', 'IBDSF', 'LDNXF', 'META', 'NVDA', 'SBGSF', 'SHOP', 'SPGI', 'TOITF', 'TTD', 'TTE', 'V'\\\n",
    "    'PATH', 'GCT', 'TMDX', 'NET', 'AXON', 'PL', 'OKTA', 'APP', 'CRWD', 'LULU', 'RKLB', 'NKE', 'AVGO', 'ZETA', 'UBER', 'SCMI', 'ISRG', 'TSM', 'ADSK', 'ARM', 'DDOG', 'HESAF', 'MDB', 'NOW', 'LVMHF', 'TWLO', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_ls = ['AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[df['symbol'].isin(st_ls)].drop_duplicates(subset=['symbol', 'date']).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'symbol': 'AMD',\n",
       "  'quarter': 2,\n",
       "  'year': 2024,\n",
       "  'date': '2024-07-30 19:28:07',\n",
       "  'content': \"Operator: Greetings, and welcome to the AMD Second Quarter 2024 Conference Call. At this time, all participants are in a listen-only mode. A brief question-and-answer session will follow the formal presentation. [Operator Instructions] As a reminder, this conference is being recorded. It is now my pleasure to introduce to you Mitch Haws, Vice President Investor Relations. Thank you, Mitch. You may begin.\\nMitch Haws : Thank you, and welcome to AMD's Second Quarter 2024 Financial Results Conference call. By now you should have had the opportunity to review a copy of our earnings press release and the accompanying slides. If you have not had the chance to review these materials, they can be found on the Investor Relations page of amd.com. We will refer primarily to non-GAAP financial measures during today's call. The full non-GAAP to GAAP reconciliations are available in today's press release, and the slides posted on our website. Participants on today's conference call are Dr. Lisa Su, our Chair and Chief Executive Officer; and Jean Hu, our Executive Vice President, Chief Financial Officer, and Treasurer. This is a live call and will be replayed via webcast on our website. Before we begin, I would like to note that Dr. Lisa Su will attend the Goldman Sachs Technology Communicopia and Technology Conference on Monday, September 9th, and Mark Papermaster, Executive Vice President and Chief Technology Officer, will attend the Deutsche Bank Technology conference on Wednesday, August 28th. Today's discussion contains forward-looking statements based on current beliefs, assumptions, and expectations. Speak only as of today and as such, involve risks and uncertainties that could cause actual results to differ materially from our current expectations. Please refer to the cautionary statement in our press release for more information on factors that could cause action results to differ materially. With that, I'll hand the call over to Lisa.\\nLisa Su : Thank you, Mitch, and good afternoon to all those listening today. We delivered strong second quarter financial results with revenue coming in above the midpoint of guidance and profitability increasing by a double-digit percentage driven by higher than expected sales of our Instinct, Ryzen, and EPYC processors. We continued accelerating our AI traction, as leading cloud and enterprise providers, expanded availability of Instinct MI300X solutions, and we also saw positive demand signals for general purpose compute in both our client and server processor businesses. As a result, second quarter revenue increased 9% year-over-year to $5.8 billion, as significantly higher sales of our data center and client processors more than offset declines in gaming and embedded product sales. We also expanded gross margin by more than 3 percentage points and grew EPS 19%, as data center product sales accounted for nearly 50% of overall sales in the quarter. Turning to the segments, data center segment revenue increased 115% year-over-year to a record $2.8 billion, driven by the steep ramp of Instinct MI300 GPU shipments and a strong double-digit percentage increase in EPYC CPU sales. Cloud adoption remains strong as hyperscalers deploy fourth-gen EPYC CPUs to power more of their internal workloads and public instances. We are seeing hyperscalers select EPYC processors to power a larger portion of their applications and workloads, displacing incumbent offerings across their infrastructure with AMD solutions that offer clear performance and efficiency advantages. The number of AMD-powered cloud instances available from the largest providers has increased 34% from a year ago to more than 900. We are seeing strong pull for these instances with both enterprise and cloud-first businesses. As an example, Netflix and Uber both recently selected fourth-gen EPYC Public Cloud instances as one of the key solutions to power their mission critical customer facing workloads. In the enterprise, sales were increased by a strong double-digit percentage sequentially. We closed multiple large wins in the quarter with financial services, technology, health care, retail, manufacturing, and transportation customers, including Adobe, Boeing, Industrial Light & Magic, Optiver, and Siemens. Importantly, more than one-third of our enterprise server wins in the first half of the year were with businesses deploying EPYC in their data centers for the first time, highlighting our success attracting new customers, while also continuing to expand our footprint with existing customers. Looking ahead, our next-generation Turin family, featuring our new Zen 5 core is looking very strong. Zen 5 is a grounds up new core design optimized for leadership performance and efficiency. Turin will extend our TCO leadership by offering up to 192 cores and 384 threads, support for the latest memory and I.O. Technologies, and the ability to drop into existing fourth-gen EPYC platforms. We publicly previewed Turin for the first time in June, demonstrating our significant performance advantages in multiple compute-intensive workloads. We also passed a major milestone in the second quarter as we started Turin production shipments to lead Cloud customers. Production is ramping now ahead of launch and we expect broad OEM and cloud availability later this year. Turning to our data center AI business, we delivered our third straight quarter of record data center GPU revenue with MI300 quarterly revenue exceeding $1 billion for the first time. Microsoft expanded their use of MI300X Accelerators to power GPT-4 Turbo and multiple co-pilot services including Microsoft 365 Chat, Word, and Teams. Microsoft also became the first large hyperscaler to announce general availability of public MI300X instances in the quarter. The new Azure VMs leverage the industry-leading compute performance and memory capacity of MI300X in conjunction with the latest ROCm software to deliver leadership-inferencing price performance when running the latest frontier models, including GPT-4. Hugging Face was one of the first customers to adopt the new Azure instances, enabling enterprise and AI customers to deploy hundreds of thousands of models on MI300X GPUs with one click. Our enterprise and Cloud AI customer pipeline grew in the quarter, and we are working very closely with our system and cloud partners to ramp availability of MI300 solutions to address growing customer demand. Dell, HPE, Lenovo, and Supermicro all have Instinct platforms in production, and multiple hyperscale and tier-2 cloud providers are on track to launch MI300 instances this quarter. On the AI software front, we made significant progress enhancing support and features across our software stack, making it easier to deploy high performance AI solutions on our platforms. We also continued to work with the open source community to enable customers to implement the latest AI algorithms. As an example, AMD support for Flash Attention 2 algorithm was upstreamed, providing out-of-the-box support for AMD hardware in the popular library that could increase training and inference performance on large transformer models. Our work with the model community also continued accelerating, highlighted by the launches of new models and frameworks with day one support for AMD hardware. At Computex, I was joined by the co-CEO of Stable Diffusion to announce that MI300 is the first GPU to support their latest SD 3.0 image generation LLM. Last week, we were proud to note that multiple partners used ROCm and MI300X to announce support for the latest Llama 3.1 models, including their 405 billion parameter version that is the industry's first frontier-level open source AI model. Llama 3.1 runs seamlessly on MI300 accelerators, and because of our leadership memory capacity, we're also able to run the FP16 version of the Llama 3.1 405B model in a single server, simplifying deployment and fine-tuning of the industry-leading model and providing significant TCO advantages. Earlier this month, we announced our agreement to acquire Silo AI, Europe's largest private AI lab with extensive experience developing tailored AI solutions for multiple enterprise and embedded customers, including Allianz, Ericsson, Finnair, Korber, Nokia, Philips, T-Mobile, and Unilever. The Silo team significantly expands our capability to service large enterprise customers looking to optimize their AI solutions for AMD hardware. Silo also brings deep expertise in large language model development, which will help accelerate optimization of AMD inference and training solutions. In addition to our acquisitions of Silo AI, [Sology] (ph), and Nod.ai, we have invested over $125 million across a dozen AI companies in the last 12 months to expand the AMD AI ecosystem, support partners, and advance leadership AMD computing platforms. Looking ahead from a roadmap perspective, we are accelerating and expanding our Instinct roadmap to deliver an annual cadence of AI accelerators, starting with the launch of MI325X later this year. MI325X leverages the same infrastructure as MI300 and extends our generative AI performance leadership by offering twice the memory capacity and 1.3 times more peak compute performance than competitive offerings. We plan to follow MI325X with the MI350 series in 2025 based on the new CDNA 4 architecture, which is on track to deliver a 35x increase in performance compared to CDNA 3. And our MI400 series powered by the CDNA “Next” architecture is making great progress in development and is scheduled to launch in 2026. Turning to our AI solutions work, Broadcom, Cisco, HP Enterprise, Intel, Google, Meta, and Microsoft all joined us to announce Ultra Accelerator Link, an industry standard technology to connect hundreds of AI accelerators that is based on AMD's proven infinity fabric technology. By combining UA-Link with the widely supported ultra-Ethernet consortium specification, the industry is coming together to establish a standardized approach for building the next generation of high-performance data centers, AI solutions at scale. In summary, customer response to our multi-year Instinct and ROCm roadmaps is overwhelmingly positive and we're very pleased with the momentum we are building. As a result, we now expect data center GPU revenue to exceed $4.5 billion in 2024, up from the $4 billion we guided in April. Turning to our client segment, revenue was $1.5 billion, an increase of 49% year-over-year driven by strong demand for our prior generation Ryzen processors and initial shipments of our next generation Zen 5 processors. In PC applications, Zen 5 delivers an average of 16% more instructions per clock than our industry leading previous generation of Ryzen processors. For desktops, our upcoming Ryzen 9000 series processors drop into existing AM5 motherboards and extends our performance and energy efficiency leadership across productivity, gaming, and content creation workloads. For notebooks, we announced our Ryzen AI 300 series that extends our industry-leading CPU and GPU performance and introduces the industry's fastest NPU with 50 tops of AI compute performance for Copilot Plus PCs. The first Ryzen AI 300 series notebooks went on sale over the weekend to strong reviews. And more than 100 Ryzen AI 300 series premium, gaming, and commercial platforms are on track to launch from Acer, ASUS, HP, Lenovo, and others over the coming quarters. Customer excitement for our new Ryzen processors is very strong, and we are well positioned for ongoing revenue share gains based on the strength of our leadership portfolio and design win momentum. Now turning to our gaming segment, revenue declined 59% year-over-year to $648 million as semi-custom SoC sales declined in-line with our projections. Semi-custom demand remains soft, as we are now in the fifth-year of the console cycle and we expect sales to be lower in the second half of the year compared to the first half. In gaming graphics, revenue increased year-over-year driven by improved sales of our Radeon 6000 and 7000 series GPUs in the channel. Turning to our embedded segment, revenue decreased 41% year-over-year to $861 million. The first quarter marked the bottom for our embedded segment revenue. Although second quarter revenue was flattish sequentially, we saw early signs of order patterns improving and expect embedded revenue to gradually recover in the second half of the year. Longer term we are building strong design win momentum for our expanded embedded portfolio. Design wins in the first half of the year increased by more than 40% from the prior year to greater than $7 billion, including multiple triple-digit million-dollar wins combining our adaptive and x86 compute products. We announced our Alveo V80 accelerators that deliver leadership capabilities in memory-intensive workloads and entered early access on next-generation Edge AI solutions with more than 30 key partners on our upcoming second-gen Versal adaptive SoCs. Last week, we also announced Victor Peng, President of AMD, would retire at the end of August. Victor has made significant contributions to Xilinx and AMD, including helping scale our embedded business and leading our cross-company AI strategy. On a personal note, Victor has been a great partner to me in sharing the success of our Xilinx acquisition and integration. On behalf of all of the AMD employees and board, I want to thank Victor for all of his contributions to AMD success and wish him all the best in his retirement. In summary, we delivered strong second quarter results and are well positioned to grow revenue significantly in the second half of the year, driven by our data center and client segments. Our data center GPU business is on a steep growth trajectory as shipments ramp across an expanding set of customers. We're also seeing strong demand for our next generation Zen 5 EPYC and Ryzen processors that deliver leadership performance and efficiency in both data center and client workloads. Looking ahead, the rapid advances in generative AI and development of more capable models are driving demand for more compute across all markets. Under this backdrop, we see strong growth opportunities over the coming years and are significantly increasing hardware, software and solutions investments with a laser focus on delivering an annual cadence of leadership data center GPU hardware, integrating industry leading AI capabilities across our entire product portfolio, enabling full stack software capabilities, amplifying our ROCm development with the scale and speed of the open source community and providing customers with turnkey solutions that accelerate the time to market for AMD based AI systems. We are excited about the unprecedented opportunities in front of us and are well positioned to drive our next phase of significant growth. Now I'd like to turn the call over to Jean to provide some additional color on our second quarter results. Jean?\\nJean Hu : Thank you Lisa, and good afternoon everyone. I'll start with a review of our financial results and then provide our current outlook for the third quarter. We're very pleased with our overall second quarter financial results that came in above expectations. On a year-over-year basis, data center segment revenue more than doubled. Client segment revenue grow significantly, and we expand the gross margin by 340 basis points. For the second quarter of 2024, revenue was $5.8 billion, up 9% year-over-year, as revenue growth in the data center and the client segments was partially offset by lower revenue in our gaming and embedded segments. Revenue increases 7% sequentially, primarily driven by growth in the data center and the client segments revenue. Gross margin was 53%, up 340 basis points year-over-year, primarily driven by higher data center revenue. Operating expenses were $1.8 billion, an increase of 15% year-over-year, as we continue to invest in R&D to address the significant AI growth opportunities ahead of us and enhanced go-to-market activities. Operating income was $1.3 billion representing a 22% operating margin. Taxes, interest expense and other was $138 million. Diluted earnings per share was $0.69, an increase of 19% year-over-year. Now turning to our reportable segment. Starting with the data center. Data Center delivered record quarterly segment revenue of $2.8 billion, up 115%, a $1.5 billion increase in year-over-year. The Data Center segment accounted for nearly 50% of total revenue, led primarily by the steep ramp of AMD Instinct GPUs and strong double-digit percentage EPYC Server revenue growth. On a sequential basis, revenue increased 21%, driven primarily by strong momentum in AMD Instinct GPUs. Data center segment operating income was $743 million or 26% of revenue compared to $147 million or 11% a year ago. Operating income was up more than 5 times from the prior year, driven by higher revenue and operating leverage, even as we significantly increase our investment in R&D. Client segment revenue was $1.5 billion, up 49% year-over-year and 9% sequentially driven primarily by AMD Ryzen processor sales. Client segment operating income was $89 million or 6% of revenue compared to operating loss of $69 million a year ago. Gaming segment revenue was $648 million down 59% year-over-year and 30% sequentially. The decrease in revenue was primarily due to semi-customer inventory digestion and the lower end-market demand. Gaming segment operating income was $77 million, or 12% of revenue compared to $225 million or 14% a year ago. Embedded segment revenue was $861 million, down 41% year-over-year, as customers continue to normalize their inventory levels. On sequential basis, embedded segment revenue was up 2%. Embedded segment operating income was $345 million or 40% of revenue compared to $757 million or 52% a year ago. Turning to the balance sheet and the cash flow. During the quarter, we generated $593 million in cash from operations and the free cash flow was $439 million. Inventory increased sequentially by [$339 million] (ph) to $5 billion, primarily to support the continued ramp of a data center GPU product. At the end of the quarter, cash, cash equivalent, and short-term investments were $5.3 billion. In the second quarter, we returned $352 million to shareholders repurchasing 2.3 million shares, and we have $5.2 billion of authorization remaining. During the quarter, we retired $750 million of debt that matured this past June utilizing existing cash. Now turning to our third quarter, 2024 outlook. We expect revenue to be approximately $6.7 billion plus or minus $300 million. Sequentially, we expect revenue to grow approximately 15%, primarily driven by strong growth in the data center and the client segment. We expect embedded segment revenue to be up and the gaming segment to decline by double digit percentage. Year-over-year we expect revenue to grow approximately 16%, driven by the steep ramp of our AMD Instinct processors and strong server and client revenue growth to more than offset the declines in the gaming and embedded segments. In addition, we expect third quarter non-GAAP gross margin to be approximately 53.5%. Non-GAAP operating expenses to be approximately $1.9 billion. Non-GAAP effective tax rate to be 13%. And the diluted share count is expected to be approximately 1.64 billion shares. Also during the third quarter, we expect to close the acquisition of Silo AI for approximately $665 million in cash. In closing, we made significant progress during the quarter toward achieving our financial goals. We delivered record MI300 revenue that exceeded $1 billion and demonstrated solid traction with our next-gen Ryzen and EPYC product. We expanded gross margin significantly and drove earnings growth while increasing investment in AI. Looking forward, opportunities ahead of us are unprecedented, will remain focused on executing to our long-term growth strategy, while driving financial discipline and operational excellence. With that, I will turn it back to Mitch for the Q&A session.\\nMitch Haws : Thank you, Jean. John, we're happy to poll the audience for questions.\\nOperator: Thank you, Mitch. We will now be conducting the question-and-answer session. [Operator Instructions] And the first question comes from the line of Ben Reitzes with Melius Research. Please proceed with your question.\\nBen Reitzes: Hey, thanks a lot. Congratulations on these results. Lisa, I wanted to ask you about MI300, how you see it playing out sequentially for the rest of the year. I guess there is about $2.8 billion left to hit your annual target. So I'm wondering if you see things picking up in the fourth quarter and how that's going sequentially. And if you don't mind, I wanted to also ask about next year if you see potential for rapid growth. You're probably aware of some of the chatter out there, and I just was wondering if you are already seeing signs that you can grow significantly, given your road map for next year. Thank you so much.\\nLisa Su: Yes. Great, Ben. Thanks for the question. So first of all on sort of MI300 and the customer evolution, we are very happy with how MI300 has progressed. When we started the year, I think the key point for us was to get our products into our customers' data centers, to have them qualify their workloads, to really ramp in production and then see what the production capabilities are especially performance and all of those things. And I can say now being sort of more than halfway through the year, we've seen great progress across the board. As we look into the second half of the year I think we would expect that MI300 revenue would continue to ramp in the third quarter and the fourth quarter. And we are continuing to expand both current deployments with our existing customers, as well as we have a large pipeline of customers that we are working through that are getting familiar with our architecture and software and all that stuff. So I’d say overall, very pleased with the progress, and really continuing right on track to what we expected from the capabilities of the product. As we go into next year, I mean one of the important things that we announced at Computex, was increasing and expanding our road map. I think we feel really good about our road map. We are on track to launch MI325 later this year. And then next year, our MI350 Series, which will be very competitive with Blackwell Solutions. And then we're well on our way to our CDNA Next as well. So I think, overall we remain quite bullish on the overall AI market. I think the market continues to need more compute. And we also feel very good that our hardware and software solutions are getting good traction, and we are continuing to expand that pipeline.\\nBen Reitzes: Thank you.\\nOperator: And the next question comes from the line of Aaron Rakers with Wells Fargo. Please proceed with your question.\\nAaron Rakers: Yeah, thanks for taking my question. And congrats on the quarter as well. I guess sticking on the Data Center side, as we look forward and you think about the full year, I'm curious of how you're currently thinking about the EPYC server CPU growth expectations as we go forward. And any kind of updated thoughts on your ability to kind of continue to gain share in the server market? Just kind of just update us on how you see the server market playing out over the next couple of quarters.\\nLisa Su: Yes, sure Aaron. Thanks for the question. So we are very pleased with the progress that we've made with EPYC. I think a couple of things. First of all, in terms of competitive positioning and just the traction in the market, our fourth-gen EPYC between Zen 1 Bergamo is really doing very well. We've seen broad adoption across cloud, and then we've been very focused on enterprise as well as third-party cloud instances. And as I said in the prepared remarks, we are starting to see very nice traction in enterprise with both new customers as well as existing customers, and then for third-party cloud adoption, also a good pickup there as well. So I think overall, I think our EPYC portfolio has done well. Going into the second half of the year, I think we also feel good about it. There are a couple of positives there. We see -- first of all the market looks like it's improving, so we have seen some return to spending in both enterprise and cloud. And so I think those are positive market trends. And then in addition to that, we are in the process of launching Turin. So we started production here in the second quarter and we are on track to launch broadly in the second half of the year. We'll see some revenue of Turin in the second half of the year contributing as well. So overall, I think the server market and our ability to continue to grow share in the server market is one of the things that we see in the second half of the year.\\nOperator: And the next question comes from the line of Timothy Arcuri with UBS. Please proceed with your question.\\nTimothy Arcuri: Thanks a lot. Lisa, I wanted to ask about the Data Center GPU roadmap. As you said, 325 launching later this year, so I guess I had two questions. Does the greater than $4.5 billion, does that include any revenue from 325? And can you talk a little bit more about 350? Obviously, we are seeing a big rack scale or shift toward rack-scale systems for the competition's product. And I'm wondering if that's what 350 is going to look like. Is it going to have liquid cooling and is it going to have a rack scale aspect to it? Thanks.\\nLisa Su: Yes, absolutely. So let me start with your original question. I mean, I think looking at 325X, we are on track to launch later this year. From a revenue standpoint there will be a small contribution in the fourth quarter but it really is still mostly the MI300 capabilities. And 325 will start in the fourth quarter and then ramp more in the first half of next year. And then as we look at the 350 Series, what we are seeing and the reason we call it a series is because there will be multiple SKUs in that series that we'll go through the range of, let's call it, air-cooled to liquid-cooled. In spending time with our customers. I think there are people who certainly want more rack level solutions, and we are certainly doing much more in terms of system-level integration for our products. You will see us invest more in system-level integration. But we also have many customers who want to use their current infrastructure. I think the beauty of the MI350 series is, it actually fits into the same infrastructure as the MI300 series. And so it would lend itself to, let's call it a pretty fast ramp if you've already invested in 300 or 325. So we see the range of options, and that's part of the expansion of the road map that we are planning.\\nOperator: And the next question comes from the line of Ross Seymore with Deutsche Bank. Please proceed with your question.\\nRoss Seymore: Hi, thanks for having me ask a question and congrats on the strong results. Well, Data Center is obviously very important. I just want to pivot to the Client side. Lisa, can you talk about the AI PC side of things? How you believe AMD is positioned? Are you seeing any competitive intensity changing with the emergence of ARM based systems? Just wanted to see how you are expecting that to roll out and what it means to second half seasonality.\\nLisa Su: Yes. Sure, Ross. So first, we are very pleased with our Client business results. I think we have a very strong road map, so I'm very pleased with the road map. The Zen 5 based products, we're launching both notebook and desktop in the middle of this year. What we've seen is actually very positive feedback on the product. So we just actually launched the first Strix-based notebooks over the weekend. They went on sale. You may have seen some of the reviews. The reviews are very positive. Our view of this is the AI PC is an important add to the overall PC category. As we go into the second half of the year, I think we have better seasonality in general, and we think we can do, let us call it above-typical seasonality, given the strength of our product launches and when we are launching. And then into 2025, you're going to see AI PCs across sort of a larger set of price points which will also open up more opportunities. So overall, I’d say, the PC market is a good revenue growth opportunity for us. The business is performing well. The products are strong. And we are working very closely with both the ecosystem partners, as well as our OEM partners to have strong launches here into the second half of the year.\\nRoss Seymore: And is the ARM side changing anything or not really?\\nLisa Su: Look, I think at this point the PC market is a big market and we are underrepresented in the market. I’d say that we take all of our competition very seriously. That being the case, I think our products are very well positioned.\\nOperator: And the next question comes from the line of Matt Ramsay with Cowen. Please proceed with your question.\\nMatt Ramsey: Thank you very much. Good afternoon. Lisa, I wanted to maybe draw a parallel between the Instinct portfolio that your company is rolling out now and what you guys did five or six years ago with EPYC. And I remember when the Naples product launched, there was a lot of, I’d say, reaction positively and negatively and sort of sentiment around where your road map might go to relatively small perturbations in what the volumes were, super early. But if I remember back to that, what was the most important was that was the toehold into the market for long-term engagement, both on the software side and the hardware side with your customers two, three, four generations forward. So is that an accurate parallel to where you guys are with MI300? And maybe you could talk about the level of engagement, the intensity of engagement, the breadth of it across the customer base with 350 and 400. Thanks.\\nLisa Su: Yes, absolutely, Matt. So look as I said earlier, we are very pleased with the progress that we are making on the Instinct road map. This is absolutely a long-term play so absolutely, you are correct. It has a lot of parallels to the EPYC journey, where you really have to -- you gain more opportunities, broader workloads, larger deployments as you go from generation to generation. So we are playing the long game here. Our conversations with our customers, so I’d start with first, in the near-term, we had some very key milestones that we wanted to pass this year. And as I said, they related to getting hardware in volume in multiple hyperscalers, as well as large Tier 2 customers. We've done that. We've now seen our software in a lot of different environments, and it is matured substantially. ROCm is in very -- from a standpoint of features, functions, out-of-box performance, getting to performance with customers, we've gained a lot of confidence and learned a lot in that whole process. The networking aspects of building out the rack scale and the system-level items are areas that we are continuing to invest in. And then the point of having long-term conversations across multiple generations is also really important. So I think all of those things have progressed well. We view this as very good progress for MI300, but we have a lot more to do. And I think the various road maps will help us open up those opportunities over the next couple of years.\\nMatt Ramsey: Appreciated. Thank you.\\nLisa Su: Thanks Matt.\\nOperator: And the next question comes from the line of Vivek Arya with Bank of America Securities. Please proceed with your question.\\nVivek Arya: Thanks for taking my question. Lisa, there seems to be this ongoing industry debate about AI monetization and whether your customers are getting the right ROI on their CapEx. And today, they have these three options, right? They can buy GPUs from your largest competitor with all the software bells and whistles and incumbency or they can do custom chips or they can buy from AMD. So how do you think this plays out next year? Do you think your customers given all this concern around monetization, does it make them consolidate their CapEx around just the other two suppliers? How is your visibility going into next year, given this industry debate? And how will AMD continue to kind of carve a position between these two other competitive choices that are out there? Thank you.\\nLisa Su: Yes, sure, Vivek. Well, I mean I think you talk to a lot of the same people that we talk to. I think the overall view on AI investment is we have to invest. I mean the industry has to invest. The potential of AI is so large to impact the way enterprises operate and all that stuff. So I think the investment cycle will continue to be strong. And then relative to the various choices for the size of the market, I firmly believe that there will be multiple solutions, whether you are talking about GPUs or you are talking about custom chips or ASICs, there will be multiple solutions. In our case, I think we've demonstrated a really strong road map and the ability to partner well with our customers. And from the standpoint of that deep engagement, hardware, software co-optimization is so important in that. And for large language models, GPUs are still the architecture of choice. So I think, the opportunity is very large. And I think our piece of that is really strong technology with strong partnerships with the key AI market makers.\\nVivek Arya: Thank you Lisa.\\nLisa Su: Thanks Vivek.\\nOperator: And the next question comes from the line of Joe Moore with Morgan Stanley. Please proceed with your question.\\nJoe Moore: Great. Thank you. I also wanted to ask about MI300. I wonder if you could talk about training versus inference. Do you have a sense -- I know that a lot of the initial focus was inference, but do you have traction on the training side? And any sense of what that split may look like over time?\\nLisa Su: Yes, sure. Thanks for the question Joe. So as we said on MI300, there are lots of great characteristics about it. One of them is our memory bandwidth and memory capacity is leading the industry. From that standpoint, the early deployments have largely been inference in most cases, and we have seen fantastic performance from an inference standpoint. We also have customers that are doing training. We've also seen that from a training standpoint, we've optimized quite a bit our ROCm software stack, to make it easier for people to train on AMD. And I do expect that we'll continue to ramp training over time. As we go forward, I think you'll see -- the belief is that inference will be larger than training from a market standpoint. But from an AMD standpoint, I’d expect both inference and training to be growth opportunities for us.\\nJoe Moore : Great. Thank you.\\nOperator: And the next question comes from the line of Toshiya Hari with Goldman Sachs. Please proceed with your question.\\nToshiya Hari: Hi, thank you so much for taking the question. I had a question on the MI300 as well. Curiously, if you are currently shipping to demand or if the updated annual forecast of $4.5 billion is in some shape or form supply constrained. I think last quarter you gave some comments on HBM and CoWoS. Curious if you could provide an update there. And then my Part B to my question is on profitability for MI300. I think in the past, you've talked about the business being accretive and improving further over time as you sort of work through the kinks, if you will. Has that view evolved or changed at all, given sort of the competitive intensity and your need to invest, whether it be through organic R&D or some of the acquisitions you've made? Or are you still confident that profit margins in the business continue to expand? Thank you.\\nLisa Su: Yes. Sure, Toshiya. Thanks for the question. So on the supply side, let me make a couple of comments and then maybe I'll let Jean comment on sort of the trajectory for the business. So on the supply side, we made great progress in the second quarter. We ramped up supply significantly exceeding $1 billion in the quarter. I think the team has executed really well. We continue to see line of sight to continue increasing supply as we go through the second half of the year. But I will say that the overall supply chain is tight and will remain tight through 2025. So under that backdrop, we have great partnerships across the supply chain. We've been building additional capacity and capability there. And so we expect to continue to ramp as we go through the year. And we'll continue to work both supply as well as demand opportunities, and really that's accelerating our customer adoption overall, and we'll see how things play out as we go into the second half of this year.\\nJean Hu: Yes. On your second question about the profitability, first our team has done a tremendous job to ramp the product MI300. It is a very complex product. So we ramped it successfully. At the same time, the team also started to implement operational optimization to continue to improve gross margin. So we continue to see the gross margin improvement. Over time, in the longer term, we do believe gross margin will be accretive to corporate average. From a profitability perspective, AMD always invests in platforms. If you look at our Data Center platform especially both the [Server] (ph) and the Data Center GPU side, we are ramping the revenue. The business model can leverage very significantly even from GPU side. Because the revenue ramp has been quite significant, the operating margin continued to expand. We definitely want to continue to invest as the opportunity is huge. At the same time, it is a profitable business already.\\nToshiya Hari: Thank you very much.\\nOperator: And the next question comes from the line of Stacy Rasgon with Bernstein Research. Please proceed with your question.\\nStacy Rasgon: Hi, guys. Thanks for taking my question. I wanted to dig into the Q3 guidance a little bit, if I could. So with Gaming down double digits, it probably means you've got close to $1 billion of growth revenue across Data Center, Client, and Embedded. I was wondering if you could give us some color on how that $1 billion-ish splits out across those three businesses. Like if I had 70% of it going to Data Center and 20% going to Client and 10% going to Embedded, like would that be like way off? Or how should we think about that apportioning out across the segment?\\nLisa Su: Yes. Maybe Stacy, let me give you the following color. So the Gaming business is down double digit as you state. Think of it as the Data Center is the largest piece of it, client next. And then on the Embedded side think of it as single-digit sequential growth.\\nStacy Rasgon: Got it. So I mean within that Data Center piece then, how does that split out? I mean, is the bulk of it [indiscernible] Instinct? Or is it sort of equally weighted between Instinct and EPYC? Or like again how does it -- again if you got, I don't know $400 million to $600 million of sequential Data Center growth or something like that, how does it split up?\\nLisa Su: Yes. So again, without being that granular, we will see both -- certainly, the Instinct GPUs will grow and we'll see also very nice growth on the server side.\\nOperator: And the next question comes from the line of Harsh Kumar with Piper Sandler. Please proceed with your question.\\nHarsh Kumar: Hi, Lisa. From my rudimentary understanding, the large difference between your Instinct products and the adoption versus your nearest competitor is kind of rack level performance and that rack level infrastructure that you may be lacking. You talked a little bit about UALink. I was wondering if you could expand on that and give us some more color on when that might -- when that gap might be closed. Or is this a major step for the industry to close that gap? Just any color would be appreciated.\\nLisa Su: Yes. So Harsh, overall, maybe if I take a step back and just talk about how the systems are evolving, there is no question that the systems are getting more complex, especially as you go into large training clusters, and our customers need help to put those together. And that includes the sort of Infinity Fabric-type solutions that are the basis for the UALink things as well as just general rack level system integration. I think what you should expect, Harsh is, first of all, we're very pleased with all of the partners that have come together for UALink. We think that's an important capability. But we have all of the pieces of this already within sort of the AMD umbrella with our Infinity Fabric, with the work with our networking capability through the acquisition of Pensando. And then you'll see us invest more in this area. So this is part of how we help customers get to market faster is by investing in all of the components, so the CPUs, the GPUs, the networking capability as well as system-level solutions.\\nHarsh Kumar: Thank you Lisa.\\nLisa Su: Thanks Harsh.\\nOperator: And the next question comes from the line of Blayne Curtis with Jefferies. Please proceed with your question.\\nBlayne Curtis: Hi, good afternoon. Thanks for taking my question. I just want to ask another question on MI300. Just curious if you can kind of characterize the makeup of the customers in the first half. I know you had, end of last year, a government customer. Is there still a government contingency? And kind of the second part of it is really you've invested in all these software assets. Kind of curious the challenge of ramping the next wave of customers. I know there's been a lot of talk on some hardware challenges, memory issues and such, but then you're investing in software. I'm sure that's a big challenge, too. Just kind of curious what the biggest hurdle is for you to kind of get that next wave of customers ramp.\\nLisa Su: Yes. So Blayne, a lot of pieces to that question, so let me try to address them. First, on your question about I think you are basically asking about the supercomputing piece. That was mainly Q4 and a bit in Q1. So if you think about our Q2 revenue, think about it as almost all AI. So it is MI300X, it's for large AI, hyperscalers as well as OEM customers going to enterprise and Tier 2 data centers. So that's the makeup of the customer set. And then in terms of the various pieces of what we're doing, I think first on your question about memory, I think there's a lot of noise in the system. I wouldn't really pay attention to all that noise in the system. I mean, this has been an incredible ramp. And I'm actually really proud of what the team has done in terms of just definitely fastest product ramp that we've ever done to $1 billion here in the -- over $1 billion in the second quarter, and then ramping each quarter in Q3 and Q4. In terms of memory, we have multiple suppliers that we've qualified on HBM3. And it is a tricky -- memory is a tricky business but I think we've done it very well and that's there. And then we are also qualifying HBM3E for future products with multiple memory suppliers as well. So to your overarching question of what are the things that we're doing, the exciting part of this is that the ROCm capability has really gotten substantially better because so many customers have been using it. And with that, what we look at is out of box performance, how long does it take a customer to get up and running on MI300? And we've seen, depending on the software that companies are using, particularly if you are based on some of the higher-level frameworks like PyTorch, et cetera, we can be out-of-the-box running very well in a very short amount of time, like let's call it, very small number of weeks. And that's great because that's expanding the overall portfolio. We’re going to continue to invest in software and that was one of the reasons that we did the Silo AI acquisition. It is a great acquisition for us, 300 scientists and engineers. These are engineers that have experience with AMD hardware and are very, very good at helping customers get up and running on AMD hardware. And that's -- so we view this as the opportunity to expand the customer base with talent like Silo AI, like Nod.ai which brought a lot of compiler talent. And then we continue to hire quite a bit organically. So I think Jean said earlier that we see leverage in the model, but we are going to continue to invest because this opportunity is huge, and we have all of the pieces. This is just about building out scale.\\nBlayne Curtis: Thanks so much.\\nLisa Su: Thanks.\\nOperator: And the next question comes from the line of Tom O'Malley with Barclays. Please proceed with your question.\\nTom O'Malley: Hi, Lisa. Thanks for taking my question. I'll give you a breather from the MI300 for a second, but just to focus on Client in the second half. No problem. Focused on Client in the second half, you kind of said above-seasonal for September, December. You're obviously launching a new notebook, desktop product, but you're also talking about AI PC. Could you just break down where you're seeing those above-seasonal trends? Is it the ASP uplift you're getting from the new products? Is it a unit assumption that's coming with AI PC? Just any kind of breakdown between those two and why you're seeing it a little bit better. Thank you.\\nLisa Su: Sure, Tom. So I think you actually said it well. We are launching Zen 5 desktops and notebooks with volume ramping in the third quarter. And that’s the primary reason that we see above-seasonal. The AI PC element is certainly 1 element of that, but there is just the overall refresh. Usually, desktop launches going into a third quarter are good for us, and we feel that the products are very well positioned. So those are the primary reasons.\\nOperator: And our final question comes from the line of Chris Danely with Citi. Please proceed with your question.\\nChris Danely: Again. Thanks for speaking me in. Just a question on gross margin. So if we look at your guidance, it seems like the incremental gross margin is dropping a little bit for Q3. Why is that happening? And then just a follow-up on another part of the gross margin angle. Have you changed your gross margin expectations for the MI300? Has the accretion point moved out a little bit?\\nJean Hu: Yes, Chris, thanks for the question. I think, first we have made a lot of progress, as you mentioned, this year to expand our gross margin from 2023 at a 50 percentage point to, we actually guided 53.5% for Q3. The primary driver is really the faster Data Center business growth. If you look at the Data Center business as a percentage of revenue from 37% in Q4 last year to now close to 50%. That faster expansion really helped us with the gross margin. When you look at the second half we will continue to see Data Center to be the major driver of our top-line revenue growth, will help with the margin expansion. But there are some other puts and takes. I think Lisa mentioned the PC business actually is going to do better in second half, especially typically, seasonally, it tends to be more consumer-focused. So that really is a little bit different dynamics there. Secondly, I’d say, Embedded business, we are going to see Embedded business to be up sequentially each quarter. But the recovery, as we mentioned earlier, is more gradual. So when you look at the balance of the picture, that's why we see the gross margin -- the pace of the gross margin changed a little bit, but we do see continued gross margin expansion. As far as MI300, we are quite confident over the long-term, it will be accretive to our corporate average. We feel pretty good about the overall Data Center business to continue to be absolutely the driver of gross margin expansion.\\nChris Danely: Thank you.\\nOperator: Thank you. I would like to turn the floor back over to Mitch for any closing comments.\\nMitch Haws: Great. That concludes today's call. Thanks to all of you for joining us today.\\nOperator: And ladies and gentlemen, that does conclude today's teleconference. You may disconnect your lines at this time. Thank you for your participation.\"},\n",
       " {'symbol': 'AMD',\n",
       "  'quarter': 1,\n",
       "  'year': 2024,\n",
       "  'date': '2024-04-30 00:00:00',\n",
       "  'content': \"Operator: Greetings, and welcome to the AMD First Quarter 2024 Conference Call. [Operator Instructions] As a reminder, this conference is being recorded. \\n It is now my pleasure to introduce your host, Mitch Haws, Vice President, Investor Relations. Thank you. Mitch, you may begin. \\nMitchell Haws: Thank you, and welcome to AMD's First Quarter 2024 Financial Results Conference Call. By now, you should have had the opportunity to review a copy of our earnings press release and the accompanying slides. If you have not had the chance to review these materials, they can be found on the Investor Relations page of amd.com. We will refer primarily to non-GAAP financial measures during today's call, and the full non-GAAP to GAAP reconciliations are available in today's press release and the slides posted on our website. \\n Participants on today's call are Dr. Lisa Su, our Chair and Chief Executive Officer; and Jean Hu, our Executive Vice President, Chief Financial Officer and Treasurer. This is a live call and will be replayed via webcast on our website. Before we begin, I would like to note that Mark Papermaster, Executive Vice President and Chief Technology Officer, will attend the TD Cowen Technology, Media and Telecom Conference on May 29; and Jean Hu, Executive Vice President, Chief Financial Officer and Treasurer, will attend the JPMorgan Global Media and Communications Conference on Tuesday, May 21; the Bank of America Global Technology Conference on Wednesday, June 5; and the Jefferies Nasdaq Investor Conference on Tuesday, June 11. \\n Today's discussion contains forward-looking statements based on current beliefs, assumptions and expectations, speak only as of today and as such, involve risks and uncertainties that could cause actual results to differ materially from our current expectations. Please refer to the cautionary statement in our press release for more information on the factors that could cause actual results to differ materially. \\n With that, I will hand the call over to Lisa. \\nLisa Su: Thanks, Mitch, and good afternoon to all those listening today. This is an incredibly exciting time for the industry as the widespread deployment of AI is driving demand for significantly more compute across a broad range of markets. Under this backdrop, we are executing very well as we ramp our data center business and enable AI capabilities across our product portfolio. \\n Looking at the first quarter, revenue increased to $5.5 billion. We expanded gross margin by more than 2 percentage points and increased profitability as Data Center and Client segment sales each grew by more than 80% year-over-year. \\n Data Center segment revenue grew 80% year-over-year and 2% sequentially to a record $2.3 billion. The substantial year-over-year growth was driven by the strong ramp of AMD Instinct MI300X GPU shipments and a double-digit percentage increase in server CPU sales. We believe we gained server CPU revenue share in the seasonally down first quarter, led by growth in enterprise adoption and expanded cloud deployments. \\n In cloud, while the overall demand environment remain mixed, hyperscalers continued adopting fourth-gen EPYC processors to power more of their internal workloads and public instances. There are now nearly 900 AMD-powered public instances available globally as Amazon, Microsoft and Google all increased their fourth-gen EPYC processor offerings with new instances and regional deployments. \\n In the enterprise, we have seen signs of improving demand as CIOs need to add more general purpose and AI compute capacity while maintaining the physical footprint and power needs of their current infrastructure. This scenario aligns perfectly with the value proposition of our EPYC processors. \\n Given our high core count and energy efficiency, we can deliver the same amount of compute with 45% fewer servers compared to the competition, cutting initial CapEx by up to half and lowering annual OpEx by more than 40%. As a result, enterprise adoption of EPYC CPUs is accelerating, highlighted by deployments with large enterprises, including American Airlines, DBS, Emirates Bank, Shell and STMicro. \\n We're also building momentum with AMD-powered solutions powering the most popular ERP and database applications. As one example of the latest generation of Oracle Exadata, the leading database solution used by 76 of the Fortune 100, is now powered exclusively by fourth-gen EPYC processors. Looking ahead, we're very excited about our next-gen Turin family of EPYC processors featuring our Zen 5 core. We're widely sampling Turin and the silicon is looking great. \\n In the cloud, the significant performance and efficiency increases of Turin position us well to capture an even larger share of both first and third-party workloads. In addition, there are 30% more Turin platforms in development from our server partners compared to fourth-gen EPYC platforms, increasing our enterprise SAM with new solutions optimized for additional workloads. Turin remains on track to launch later this year. \\n Turning to our broader Data Center portfolio. We delivered our second straight quarter of record data center GPU revenue as MI300 became the fastest-ramping product in AMD history, passing $1 billion in total sales in less than 2 quarters. In cloud, MI300x production deployments expanded at Microsoft, Meta and Oracle to power Generative AI training and inferencing for both internal workloads and a broad set of public offerings. \\n For the enterprise, we're working very closely with Dell, HPE, Lenovo, Super Micro and others as multiple MI300X platforms enter volume production this quarter. In addition, we have more than 100 enterprise and AI customers actively developing or OpenAI MI300X. \\n On the AI software front, we made excellent progress adding upstream support for AMD hardware in the open AI Triton compiler, making it even easier to develop highly performant AI software for AMD platforms. \\n We also released a major update to our ROCm software stack that expand support for open source libraries, including vLLM, and frameworks, including JAKs, adds new features like video decode and significantly increases generative AI performance by integrating advanced attention algorithm support for sparsity and FPA. \\n Our partners are seeing very strong performance in their AI workloads. As we jointly optimize for their models, MI300X GPUs are delivering leadership inferencing performance and substantial TCO advantages compared to H100. For instance, several of our partners are seeing significant increases in tokens per second when running their flagship LLMs on MI300x compared to H100. \\n We're also continuing to enable the broad ecosystem required to power the next generation of AI systems, including as a founding member of the Ultra Ethernet Consortium, working to optimize the widely adopted Ethernet protocol to run AI workloads at data center scale. \\n MI300 demand continues to strengthen. And based on our expanding customer engagements, we now expect data center GPU revenue to exceed $4 billion in 2024, up from the $3.5 billion we guided in January. Longer term, we are increasingly working closer with our cloud and enterprise customers as we expand and accelerate our AI hardware and software road maps and grow our data center GPU footprint. \\n Turning to our Client segment. Revenue was $1.4 billion, an increase of 85% year-over-year, driven by strong demand for our latest-generation Ryzen mobile and desktop processors with OEMs and in the channel. Client segment revenue declined 6% sequentially. \\n We saw strong demand for our latest-generation Ryzen processors in the first quarter. Ryzen desktop CPU sales grew by a strong double-digit percentage year-over-year, and Ryzen mobile CPU sales nearly doubled year-over-year as new Ryzen 8040 notebook designs from Acer, Asus, HP, Lenovo and others ramped. \\n We expanded our portfolio of leadership enterprise PC offerings with the launch of our Ryzen Pro 8000 processors earlier this month. Ryzen Pro 8040 mobile CPUs delivered industry-leading performance in battery life for commercial notebooks. And our Ryzen Pro 8000 series desktop CPUs are the first processor to offer dedicated, on-chip AI accelerators in commercial desktop PCs. \\n We see clear opportunities to gain additional commercial PC share based on the performance and efficiency advantages of our Ryzen Pro portfolio and an expanded set of AMD-powered commercial PCs from our OEM partners. Looking forward, we believe the market is on track to return to annual growth in 2024, driven by the start of an enterprise refresh cycle and AI PC adoption. \\n We see AI as the biggest inflection point in PC since the Internet, with the ability to deliver unprecedented productivity and usability gains. We're working very closely with Microsoft and a broad ecosystem of partners to enable the next generation of AI experiences, powered by Ryzen processors, with more than 150 ISVs on track to be developing for AMD AI PCs by the end of the year. \\n We will also take the next major step in our AI PC road map later this year with the launch of our next-generation Ryzen mobile processors codenamed Strix. Customer interest in Strix is very high based on the significant performance and energy efficiency uplifts we are delivering. \\n Design win momentum for premium notebooks is outpacing prior generations as Strix enables next-generation AI experiences in laptops that are thinner, lighter and faster than ever before. We're excited about the growth opportunities for the PC market. And based on the strength of our Ryzen CPU portfolio, we expect to grow revenue share this year. \\n Now turning to our Gaming segment. Revenue declined 48% year-over-year and 33% sequentially to $922 million. First quarter semi-custom SoC sales declined in line with our projections as we are now in the fifth year of the console cycle. In Gaming Graphics, revenue declined year-over-year and sequentially. We expanded our Radeon 7000 Series family with the global launch of our Radeon RX 7900 GRE and also introduced our driver-based AMD fluid motion frames technology that can provide large performance increases in thousands of games. \\n Turning to our Embedded segment. Revenue decreased 46% year-over-year and 20% sequentially to $846 million as customers remain focused on normalizing their inventory levels. We launched our Spartan UltraScale+ FPGA family with high I/O counts, power efficiency and state-of-the-art security features, and we're seeing a strong pipeline of growth for our cost-optimized embedded portfolio across multiple markets. \\n Given the current embedded market conditions, we're now expecting second quarter embedded segment revenue to be flat sequentially, with a gradual recovery in the second half of the year. Longer term, we see AI at the edge as a large growth opportunity that will drive increased demand for compute across a wide range of devices. To address this demand, we announced our second generation of Versal adaptive SoCs that deliver a 3x increase in AI tops per watt and a 10x greater scaler compute performance compared to our prior generation of industry-leading adaptive SoCs. \\n Versal Gen 2 adaptive SoCs are the only solution that combine multiple compute engines to handle AI preprocessing, inferencing and post processing on a single chip, enabling customers to rapidly add highly performant and efficient AI capabilities to a broad range of products. We were pleased to be joined at our launch by Subaru, who announced they adopted Versal AI Edge series Gen 2 devices to power the next generation of their EyeSight ADAS system. \\n Embedded Design win momentum remains very strong as customers adopt our full portfolio of FPGAs, CPUs, GPUs and adaptive SoCs to address a larger portion of their compute needs. In summary, we executed well in the first quarter, setting us up to deliver strong annual revenue growth and expanded gross margin, driven by growing adoption of our Instinct, EPYC and Ryzen product portfolios. \\n Our priorities for 2024 are very clear: accelerate our Data Center growth by ramping Instinct GPU production and gaining share with our EPYC processors; launch our next-generation Zen 5 PC and server processors that extend our leadership performance; and expand our adaptive computing portfolio with differentiated solutions. \\n Looking further ahead, AI represents an unprecedented opportunity for AMD. While there has been significant growth in AI infrastructure build-outs, we are still in the very early stages of what we believe is going to be a period of sustained growth, driven by an insatiable demand for both high-performance AI and general purpose compute. \\n We have expanded our investments across the company to capture this large growth opportunity, from rapidly expanding our AI software stack to accelerating our AI hardware road maps, increasing our go-to-market activities and partnering closely with the largest AI companies to co-optimize solutions for their most important workloads. We are very excited about the trajectory of the business and the significant growth opportunities ahead. \\n Now I'd like to turn the call over to Jean to provide some additional color on our first quarter results. Jean? \\nJean Hu: Thank you, Lisa, and good afternoon, everyone. I'll start with a review of our financial results and then provide our current outlook for the second quarter of fiscal 2024. We delivered strong year-over-year revenue growth in our Data Center and Client segments in the fourth quarter and grew 230 basis points of gross margin expansion. For the first quarter of 2024, revenue was $5.5 billion, up 2% year-over-year as revenue growth in the Data Center and the Client segment was partially offset by lower revenue in our Gaming and Embedded segment. \\n Revenue declined 11% sequentially as higher Data Center revenue resulting from the ramp of our AMD Instinct GPUs was offset by lower Gaming and Embedded segment revenues. Gross margin was 52%, up 230 basis points year-over-year, driven by higher revenue contribution from the Data Center and Client segment, partially offset by lower Embedded and Gaming segment revenue contribution. \\n Operating expenses were $1.7 billion, an increase of 10% year-over-year, as we continued investing aggressively in R&D and marketing activities to address the significant AI growth opportunities ahead of us. Operating income was $1.1 billion, representing a 21% operating margin. Taxes, interest expense and other was $120 million. For the fourth quarter of 2024, diluted earnings per share was $0.62, an increase of 3% year-over-year. \\n Now turning to our Reportable segment, starting with the Data Center. Data Center delivered record quarterly segment revenue of $2.3 billion, up 80%, a $1 billion increase year-over-year. Data Center accounted for more than 40% of total revenue, primarily led by the ramp of AMD Instinct GPUs from both cloud and enterprise customers and a strong double-digit percentage growth in our server process revenue as a result of growth across our sample products. \\n On a sequential basis, revenue increased 2%, driven by the ramp of our AMD Instinct GPUs, partially offset by seasonal decline in server CPU sales. Data Center segment operating income was $541 million or 23% of revenue compared to $148 million or 11% a year ago. Operating income was up 266% year-over-year due to operating leverage even as we significantly increased our investment in R&D. \\n Client segment revenue was $1.4 billion, up 85% year-over-year, driven primarily by Ryzen 8000 series processors. On a sequential basis, Client revenue declined 6%. Client segment operating income was $86 million or 6% of revenue compared to an operating loss of $172 million a year ago, driven by higher revenue. \\n Gaming segment revenue was $922 million, down 48% year-over-year and down 33% sequentially due to a decrease in semi customer and Radeon GPU sales. Gaming segment operating income was $151 million or 16% of revenue compared to $314 million or 18% a year ago. \\n Embedded segment revenue was $846 million, down 46% year-over-year and 20% sequentially as customers continue to manage their inventory levels. Embedded segment operating income was $342 million or 41% of revenue compared to $798 million or 51% a year ago. \\n Turning to the balance sheet and cash flow. During the quarter, we generated $521 million in cash from operations, and free cash flow was $379 million. Inventory increased sequentially by $301 million to $4.7 billion, primarily to support the continued ramp of data center and client products in advanced process node. \\n At the end of the quarter, cash, cash equivalent and short-term investment was $6 billion. As a reminder, we have $750 million of debt maturing this June. Given our ample liquidity, we plan to retire that utilizing existing cash. \\n Now turning to our second quarter 2024 outlook. We expect revenue to be approximately $5.7 billion, plus or minus $300 million. Sequentially, we expect Data Center segment revenue to increase by double-digit percentage, primarily driven by the Data Center GPU ramp; client segment revenue to increase; embedded segment revenue to be flat; and in the Gaming segment, based on current demand signals, revenue to decline by significant double-digit percentage. \\n Year-over-year, we expect our Data Center and Client segment revenue to be up significantly, driven by the strength of our product portfolio; the Embedded and the Gaming segment revenue to decline by a significant double-digit percentage. In addition, we expect second quarter non-GAAP gross margin to be approximately 53%. Non-GAAP operating expenses to be approximately $1.8 billion. Non-GAAP effective tax rate to be 13% and the diluted share count is expected to be approximately 1.64 billion shares. \\n In closing, we started the year strong. We made significant progress on our strategic priorities, delivering year-over-year revenue growth in our Data Center and the Client segment and expanded the gross margin. Looking ahead, we believe the investments we are making will position us very well to address the large AI opportunities ahead. \\n With that, I'll turn it back to Mitch for the Q&A session. \\nMitchell Haws: Thank you, Jean. Paul, we're happy to poll the audience for questions. \\nOperator: [Operator Instructions] Our first question is from Toshiya Hari with Goldman Sachs. \\nToshiya Hari: Lisa, my first question is on the MI300. You're taking up the full year outlook from $3.5 billion to $4 billion. I'm curious what's driving that incremental $500 million in revenue? Is it new customers? Is it additional bookings from existing customers? Is it more cloud? Is it more enterprise? If you can sort of provide color there, that would be helpful. \\n And then on the supply side, there's been headlines or chatter that CoWoS and/or HBM could be a pretty severe constraining factor for you guys. If you can speak to how you're handling the supply side of the equation, that would be helpful, too. And then I have a quick follow-up. \\nLisa Su: Great. Thank you, Toshiya, for the question. Look, the MI300 ramp is going really well. If we look at just what's happened over the last 90 days, we've been working very closely with our customers to qualify MI300 in their production data centers, both from a hardware standpoint, software standpoint. So far, things are going quite well. \\n And what we see now is just greater visibility to both current customers as well as new customers committing to MI300. So that gives us the confidence to go from $3.5 billion to $4 billion. And I view this as very much -- it's a very dynamic market, and there are lots of customers. We said on the -- in the prepared remarks that we have over 100 customers that we're engaged with in both development as well as deployment. So overall, the ramp is going really well. \\n As it relates to the supply chain, actually, I would say, I'm very pleased with how supply has ramped. It is absolutely the fastest product ramp that we have done. It's a very complex product, chiplets, CoWoS, 3D integration, HBM. And so far, it's gone extremely well. We've gotten great support from our partners. And so I would say, even in the quarter that we just finished, we actually did a little bit better than expected when we first started the quarter. \\n I think Q2 will be another significant ramp. And we're going to ramp supply every quarter this year. So I think the supply chain is going well. We are tight on supply. So there's no question in the near term that if we had more supply, we have demand for that product, and we're going to continue to work on those elements as we go through the year. But I think both on the demand side and the supply side, I'm very pleased with how the ramp is going. \\nToshiya Hari: And then as my follow-up, I was hoping you could speak to your Data Center GPU road map beyond the MI300. The other concern that we hear is your nearest competitor has been pretty transparent with their road map, and that extends into '25 and oftentimes '26. So -- and maybe this isn't the right venue for you to give too much, but beyond the MI300, how should we think about your road map and your ability to compete in Data Center? \\nLisa Su: Yes, sure. So look, Toshiya, when we start with the road map, I mean, we always think about it as a multiyear, multigenerational road map. So we have the follow-ons to MI300 as well as the next, next generations well in development. I think what is true is we're getting much closer to our top AI customers. They're actually giving us significant feedback on the road map and what we need to meet their needs. \\n Our chiplet architecture is actually very flexible. And so that allows us to actually make changes to the road map as necessary. So we're very confident in our ability to continue to be very competitive. Frankly, I think we're going to get more competitive. Right now, I think MI300x is in a sweet spot for inference, very, very strong inference performance. \\n I see as we bring in additional products later this year into 2025, that, that will continue to be a strong spot for us. And then we're also enhancing our training performance and our software road map to go along with it. So more details to come in the coming months, but we have a strong road map that goes through the next couple of years, and it is informed by just a lot of learning in working with our top customers. \\nOperator: Our next question is from Ross Seymore with Deutsche Bank. \\nRoss Seymore: The non-AI side of the Data Center business, it sounds like the enterprise side has some good traction even though the sequential drop happened seasonally, Lisa. But I was just wondering what's implied in your second quarter guidance for the Data Center CPU side of things? And generally speaking, how are you seeing that whole kind of GPU versus CPU crowding out dynamic playing out for the rest of 2024? \\nLisa Su: Yes, sure, Ross, thanks for the question. I think the -- our EPYC business has actually performed pretty well. The market is a bit mixed. I think some of the cloud guys are still working through sort of their optimizations. I think it's different by customer. We did see here in the first quarter, actually, some very nice early signs in the enterprise space, sort of large customers starting refresh programs. The value proposition of Genoa is very, very strong, and we're seeing that pull through across the enterprise. \\n In the second quarter, we expect overall Data Center to be up strong double digits. And then within that, we expect server to be up as well. And as we go into the second half of the year, I think there are a couple of drivers for us. \\n We do expect some improvement in the overall market conditions for the server business. But we also have our Turin launch in the second half of the year that will also, we believe, extend our leadership position within the server market. So overall, I think the business is performing well, and we believe that we're continuing to be very well positioned to gain share throughout the year. \\nRoss Seymore: And I guess as my follow-up, just switching over to the Client side. I noted you guided it up sequentially. Any sort of magnitude around that for the second quarter? And perhaps, more importantly, when you talk about the whole AI PC side of things, do you believe that's more of a units driver for you, an SAP driver, or will it be both? \\nLisa Su: Yes. So I think, again, I think we're pretty excited about the AI PC, both opportunity in, let's call it, the near term and even more so in the medium term. I think the client business is performing well, both on the channel and on the MNC side. We expect clients to be up sequentially in the second quarter. \\n And as we go into the second half of the year, to your question about units versus ASPs, I think we expect some increase in units as well as ASPs. The AI PC products, when we look at the Strix products, they're really well-suited for the premium segments of the market. And I think that's where you're going to see some of the AI PC content strongest in the beginning. And then as we go into 2025, you would see it more across the rest of the portfolio. \\nOperator: Our next question is from Matt Ramsay with TD Cowen. \\nMatthew Ramsay: Lisa, I have sort of a longer-term question and then a shorter-term follow-up one. I guess the -- one of the questions that I've been getting from folks a lot is, obviously, your primary competitor has announced, I guess, a multiyear road map. And we continue to hear more and more from other folks about internal ASIC programs at some of your primary customers, whether they be for inference or training or both. \\n I guess it'd be really helpful if you could just talk to us about how your conversations go with those customers, how committed they are to your long-term road map, multigeneration, as you described it, how they juxtapose doing investments of their internal silicon versus using a merchant supplier like yourselves and maybe what advantages the experience across a large footprint of customers can give your company that those guys doing internal ASICs might not get? \\nLisa Su: Yes. Sure, Matt. Thanks for the question. So look, I think one of the things that we see and we've said is that the TAM for AI compute is growing extremely quickly. And we see that continuing to be the case in all conversations. We had highlighted a TAM of let's call it, $400 billion in 2027. I think some people thought that was aggressive at the time. But the overall AI compute needs, as we talk to customers is very, very strong. And you've seen that in some of the announcements even recently with some of the largest cloud guys. \\n From my view, there are several aspects of it. First of all, we have great relationships with all of sort of the top AI companies. And the idea there is we want to innovate together. When you look at these large language models and everything that you need for training and inferencing there, although -- there will be many solutions. I don't think there's just one solution that will fit all. The GPU is still the preferred architecture, especially as the algorithms and the models are continuing to evolve over time. And that favors our architecture and also our ability to really optimize CPU with GPU. \\n So from my standpoint, I think we're very happy with the partnerships that we have. I think this is a huge opportunity for all of us to really innovate together. And we see that there's a very strong commitment to working together over multiple years going forward. And that's, I think, a testament to some of the work that we've done in the past, and that very much is what happened with the EPYC road map as well. \\nMatthew Ramsay: Lisa, as my follow-up, a little bit shorter term. And I guess, having followed the company super closely for a long time, I think there's been -- there's always been noise in the system from whether the stock price is $2 a share or $200. There's been kind of always consistent noise one way or the other, but the last 1.5 months has been extreme in that sense. \\n And so I wanted to just -- I got random reports by inbox about changes in demand from some of your MI300 customers or planned demand for consuming your product. I think you answered earlier about the supply situation and how you're working with your partners there. But has there been any change from the customers that you're in ramp with now or that you soon will be of what their intention is for demand? Or in fact, has that maybe strengthened rather than gone down in recent periods because I keep getting questions about it? \\nLisa Su: Sure, Matt. Look, I think I might have said it earlier, but maybe I'll repeat it again. I think the demand side is actually really strong. And what we see with our customers and what we are tracking very closely is customers moving from, let's call it, initial POCs to pilots to full-scale production to deployment across multiple workloads. And we're moving through that sequence very well. I feel very good about the deployments and ramps that we have ongoing right now. \\n And I also feel very good about new customers who are sort of earlier on in that process. So from a demand standpoint, we continue to build backlog as well as build engagements going forward. And similarly, on the supply standpoint, we're continuing to build supply momentum. But from a speed of ramp standpoint, I'm actually really pleased with the progress. \\nOperator: Our next question is from Aaron Rakers with Wells Fargo. \\nAaron Rakers: I apologize if I missed this earlier, but I know last quarter, you talked about having a -- securing enough capacity to support significant upside to the ramp of the MI300. I know that you upped your guide now to $4 billion. I'm curious how you would characterize the supply relative to that context offered last quarter as we think about that new kind of target for? Would you characterize it as still having supply capacity upside potential? \\nLisa Su: Yes, Aaron. So we've said before that our goal is to ensure that we have supply that exceeds the current guidance, and that is true. So as we've upped our guidance from $3.5 billion to $4 billion, we still -- we have supply visibility significantly beyond that. \\nAaron Rakers: Yes. Okay. And then as a quick follow-up, going back to an earlier question on server demand, more traditional server. As you see the ramp of maybe share opportunities in more traditional enterprise, I'm curious how you would characterize the growth that you expect to see a more traditional server CPU market as we move through '24 or even longer term, how you'd characterize that growth trend? \\nLisa Su: Yes. I think, Aaron, what I would say is there are -- the need for refresh of, let's call it, older equipment is certainly there. So we see a refresh cycle coming. We also see AI head nodes as another place where we see growth in, let's call it, the more traditional SSD market. Our sweet spot is really in the highest performance, sort of high core count, energy efficiency space, and that is playing out well. \\n And we're also -- we've traditionally been very strong in, let's call it, cloud first-party workloads, and that is now extending to cloud third-party workloads, where we see enterprises who are, let's call it, in more of a hybrid environment, adopting AMD both in the cloud and on-prem. So I think, overall, we see it as a continued good progression for us with the server business going through 2024 and beyond. \\nOperator: Our next question is from Vivek Arya with Bank of America Securities. \\nVivek Arya: Lisa, I just wanted to go back to the supply question and the $4 billion outlook for this year. I think at some point, there was a suggestion that the $4 billion number, right, that there are still supply constraints. But I think at a different point, you said that you have supply visibility significantly beyond that. \\n Given that we are almost at the middle of the year, I would have thought that you would have much better visibility about the back half. So is the $4 billion number a supply-constrained number, or is it a demand-constrained number? Or alternatively, if you could give us some sense of what the exit rate of your GPU sales could be. I think on the last call, $1.5 billion was suggested. Could it be a lot more than that in terms of your exit rate of MI for this year? \\nLisa Su: Yes. Vivek, let me try to make sure that we answered this question clearly. From a full year standpoint, our $4 billion number is not supply capped -- I'm sorry, yes, it's not supply capped. It is -- we do have supply capability above that. It is more back half weighted. So if you're looking at sort of the near term, I would say, for example, in the second quarter, we do have more demand than we have supply right now, and we're continuing to work on pulling in some of that supply. \\n By the way, I think this is an overall industry issue. This is not at all related to AMD. I think overall, AI demand has exceeded anyone's expectations in 2024. So you've heard it from the memory guys. You've heard it from the foundry guys. We're all ramping capacity as we go through the year. \\n And as it relates to visibility, we do have good visibility into what's happening. As I said, we have great customer engagements that are going forward. My goal is to make sure that we pass all of the milestones as we're ramping products. And as we pass those milestones, we put that into the overall full year guidance for AI. \\n But in terms of how customer progression, things are going, they're actually going quite well. And we continue to bring new customers on, and we continue to expand workloads with our current customers. And so hopefully, that clarifies the question, Vivek. \\nVivek Arya: Maybe one, not on MI, but maybe on the Embedded business. I think you sound a bit more measured about Q2 and the second half rebound, which is similar to what we have heard from a lot of the auto industrial peers. But where are you in the inventory clearing cycle? And if Embedded has a somewhat more measured rebound in the back half, what implication does that have on gross margin expansion? Can we continue to expect, I don't know, 100 basis points a quarter in terms of gross margin expansion because of the Data Center mix? Or just any puts and takes of Embedded and then what it means for gross margins in the back half? \\nJean Hu: Vivek, thank you for the question. I think the Embedded business declined a little bit more than expected, really due to the weaker demand in some of the markets, very specifically, communication has been weak. And some pockets of industrial and automotive, as you mentioned, it's actually quite consistent with the peers. \\n Second half, we do think the first half is the bottom of Embedded business and will start to see gradual recovery in the second half. And going back to your gross margin question is, when you look at our gross margin expansion in both Q1 and the guide at Q2, the primary driver is the strong performance on the Data Center side. The Data Center will continue to ramp in second half. I think that will continue to be the major driver of gross margin expansion in second half. Of course, if Embedded is doing better, we'll have a more tailwind in the second half. \\nOperator: Our next question is from Timothy Arcuri with UBS. \\nTimothy Arcuri: I also wanted to ask about your data center GPU road map. The customers that we talk to say that they're engaged, not just because of MI300, but really because of what's coming. And it seems like there's a big demand shift to rack scale systems that try to optimize performance per square foot given some of the data center and power constraints. So can you just talk about how important systems are going to be in your road map? And do you have all the pieces you need as the market shifts to rack scale systems? \\nLisa Su: Yes, sure, Timothy. Thanks for the question. For sure, look, our customers are engaged in the multigenerational conversation. So we're definitely going out over the next couple of years. And as it relates to the overall system integration, it is quite important. It is something that we're working very closely with our customers and partners on. That's a significant investment in networking, working with a number of networking partners as well to make sure that the scale-out capability is there. \\n And to your question of do we have the pieces? We do absolutely have the pieces, I think the work that we've always done with our Infinity Fabric as well as with our Pensando acquisition that's brought in a lot of networking expertise. And then we're working across the networking ecosystem with key partners like Broadcom and Cisco and Arista, who are with us at our AI data center event in December. \\n So our work right now in future generations is not just specifying a GPU, it is specifying, let's call it, full system reference designs. And that's something that will be quite important going forward. \\nTimothy Arcuri: And then just as a quick follow-up. I know this year it looks like it's going to be pretty back-half loaded in your server CPU business, just like it was last year. I know you kind of held our hands at about this time last year sort of on what the full year could look like and how back-end loaded it could be. \\n So I kind of wonder, could you give us some milestones in terms of how much server CPU could grow this year, how back-end loaded it could be? Is it like up 30% this year for your server CPU business year-over-year? Is that a reasonable bogey? I just wonder if you can kind of give us any guidance on that piece of the business? \\nLisa Su: Yes. I mean, I think, Tim, I think the best way to say it is our Data Center segment is on a very, very strong ramp as we go through the back half of the year. Server CPUs, certainly, Data Center GPUs, for sure. So I don't know that we're going to get into specifics, but I could say, in general, you should expect overall at the segment level to be very strong double digits. \\nOperator: Our next question is from Joe Moore with Morgan Stanley. \\nJoseph Moore: I wonder if you could address the profitability of MI300. I know you said a couple of quarters ago that it would eventually be above corporate average, but it would take you a few quarters to get there. Can you talk about where you are in that? \\nJean Hu: Yes. Thank you, Joe. Our team has done an incredible job to ramp MI300. As you probably know, it's a very complex product, and we are still at the first year of the ramp, both from yield, the testing time and the process improvement, those things are still ongoing. We do think over time, the gross margin should be accretive to corporate average. \\nJoseph Moore: Great. And then as a separate follow-up. On the Turin transition on server, I know when you had transitioned in generally, you said it could take a little while, that there were significant platform shifts and things like that. Turin seems to be much more kind of ecosystem compatible. How quickly do you think you might see that product ramp within our server portfolio? \\nLisa Su: Yes. Joe, I think from what we see, look, think Turin is the same platform so that does make it an easier ramp. I do think that Genoa and Turin will coexist for some amount of time because customers are deciding when they're going to bring out their new platforms. We expect Turin to give us access to a broader set of workloads. So our SAM actually expands with Turin, both in enterprise and cloud. And from our experience, I think you'll see a faster transition than, for example, when we went from Milan to Genoa. \\nOperator: Our next question is from Stacy Rasgon with Bernstein Research. \\nStacy Rasgon: For my first one, I wanted to address the MI300 ramp into Q2. So you said you've done $1 billion, give or take, in cumulative sales, which puts it at maybe, I don't know, maybe $600 million in Q1. You're guiding total revenues up about $225 million into Q2, but you've got Client up, you've got traditional Data Center up, you've got Embedded flat. Gaming is going to be down, but I'd hazard a guess that the client and traditional Data Center offset it, if not more. Does the MI300 ramp into Q2? Is it more or less than the total corporate ramp that you've got built into guidance right now that you're expecting? \\nJean Hu: Stacy, thanks for the question. You always ask a math question. So I think, in general, it is more. The Data Center GPU ramp, it will be more than the overall company's $200-some million ramp. \\nStacy Rasgon: Okay. So that means Gaming must be down like a lot, right, if client [indiscernible] \\nJean Hu: Yes, yes, you're right. Gaming is down similar zip code like  Q1. \\nStacy Rasgon: Got it. Got it. That's helpful. \\nJean Hu: So maybe -- yes, maybe let me give you some color about the Gaming business, right? If you look at the Gaming, the demand has been quite weak, that's quite very well-known and also their inventory level. So based on the visibility we have, the first half, both Q1, Q2, we guided down sequentially more than 30%. We actually think the second half will be lower than first half. \\n That's basically how we're looking at this year for the Gaming business. And at the same time, Gaming's gross margin is lower than our company average. So overall, will help the mix on the gross margin side. That's just some color on the Gaming side. But you're right, Q2 Gaming is down a lot. \\nStacy Rasgon: Got it. That's helpful. For my second question, I wanted to look at the near-term Data Center profitability. So operating profit was down 19% sequentially on 2% revenue growth. Is that just the margins of the GPUs filtering in relative to the CPUs? And I know you said GPUs would eventually be above corporate average. Are they below the CPU average? I mean they clearly are, I guess, in the near term, but are they going to stay that way? \\nJean Hu: Yes. I think you're right. It's -- the GPU gross margin right now is below the Data Center gross margin level, I think there are 2 reasons. Actually, the major reason is we actually increased the investment quite significantly to, as Lisa mentioned, to expand and accelerating our road map in the AI side. That's one of the major drivers for the operating income coming down slightly. \\n On the gross margin side, going back to your question, we said in the past, and we continue to believe the case is, Data Center GPU gross margin over time will be accretive to corporate average. But it will take a while to get to the Server level for gross margin. \\nOperator: Our next question is from Harlan Sur with JPMorgan. \\nHarlan Sur: On your Data Center GPU segment and the faster time to production shipments, given you just upped your full year GPU outlook, how much of it is faster bring up of your customers' frameworks driven by your latest ROCm software platform and maybe stronger collaboration with your customers' engineers just to get them to call faster? And how much of it is just a more aggressive build-out plan by customers versus their prior expectations given what appears to be a pretty strong urgency for them to move forward with their important AI initiatives? \\nLisa Su: Yes. Harlan, thank you for the question. What it really is, is both us and our customers feeling confident in broadening the ramp? Because if you think about it, first of all, the ROCm stock has done really well. And the work that we're doing is hand in hand with our customers to optimize their key models. And it was important to get sort of verification and validation that everything would run well, and we've now passed some important milestones in that area. .\\n And then I think the other thing is, as you said, there is a huge demand for more AI compute. And so our ability to participate in that and help customers get that up and running is great. So I think, overall, as we look at it, this ramp has been very, very aggressive as you think about where we were just a quarter ago. Each of these are pretty complex bring ups. And I'm very happy with how they've gone. And by the way, we're only sitting here in April. So there's still a lot of 2024 to go, and there's a great customer momentum in the process. \\nHarlan Sur: Yes, absolutely. Just going back, just kind of rewinding back to the March quarter. So similar to the PC Client business, right, which declined at the low end of the seasonal range, if I make certain assumptions around your Data Center GPU business, x that out of Data Center, it looks like your Server CPU business was also down at the lower end of the seasonal range. By my math, it was down like 5%, 6% sequentially. Is that right? \\n And that's less than half the decline of your competitor. And if so, like what drove the less-than-seasonal declines? I assume some of it was share gains. It sounds like Enterprise was also better. Looks like you guys did drive a little bit more cloud instance adoption, but anything else that drove to a slightly better seasonal pattern in March for Data Center? Server? \\nJean Hu: Yes. Harlan, this is Jean. I think the Server business has been performing really well. Year-over-year, it actually increased a very strong double digit. I think, sequentially, it is more seasonal, but we feel pretty good about continue gaining share there. \\nLisa Su: Yes. And if I'd just add, Harlan, to your question, we did see strength in enterprise in the first quarter. And I think that has -- that offset perhaps some of the normal seasonality. \\nOperator: Our next question is from Tom O'Malley with Barclays. \\nThomas O'Malley: I just wanted to ask on the competitive environment. Obviously, on the CPU side, you had a competitor talk about launching a high core count product in the coming quarter, kind of ramping now and more so into Q3. You've seen really good pricing tailwinds as a function of the higher core capital. Can you talk about what you're seeing in that market? Do you think that there's any risk for more aggressive pricing, which would impact your ASP ramp for the rest of the year? \\nLisa Su: Yes. When we look at our server CPU sort of ASPs, they're actually very stable. I think we -- again, we tend to be indexed towards the higher core counts. Overall, I would say, the pricing environment is stable. This is about sort of TCO for sort of the customer environment and sort of our performance and our performance per watt, our leadership. And that usually translates into TCO advantage for our customers. \\nThomas O'Malley: Helpful. And then just a broader question to follow up here. So I think you got asked earlier about the importance of systems. But on your end, how important is the Open Ethernet Consortium to you being able to move forward to systems? I know that, today, you obviously have some internal assets and then you can partner with others. But is there a way that you could be competitive before there is an industry standard on the Ethernet side? And can you talk about when you think the timing of that kind of consortium comes to market and enables you to maybe accelerate that road map? \\nLisa Su: Yes. I think it's very important to say we are very supportive of the open ecosystem. We're very supportive of the Ultra Ethernet Consortium. But I don't believe that, that is a limiter to our ability to build large-scale systems. I think Ethernet is something that many in the industry feel will be the long-term answer for networking in these systems, and we have a lot of work that we're doing with internally as well as with our customers and partners to enable that. \\nOperator: Our last question is from Harsh Kumar with Piper Sandler. \\nHarsh Kumar: Lisa, I had two. One is for you and one perhaps for Jean. So we recently hosted a very large custom GPU company for a call. And they talked about kind of mega data centers coming up in the near to midterm, talking about nodes potentially in the 100,000-plus range and maybe up to 1 million. So as we look out at these kinds of data centers, from an architectural standpoint, it's not a situation where winner takes all, where if somebody gets in, they kind of get all the sockets? \\n Or will there reliance where your chip perhaps or your board can be placed right next to somebody else's board maybe on a separate line? Just help us understand how something like that would play out if there's a  chance for more than 1 competitor to play in such a large data center? \\nLisa Su: Yes. So I'll talk maybe a little bit more at the strategic level. I think as we look at sort of how AI shapes up over the next few years, there are customers who would be looking at very large training environments and perhaps that's what you're talking about. I think our view of that is, number one, we view that as a very attractive area for AMD. It's an area where we believe we have the technology to be very competitive there. \\n And I think the desire would be to have optionality in terms of how you build those out. So obviously, a lot has to happen between here and there. But I think your overarching question of. Is it winner takes all? I don't think so. That being the case, we believe that AMD is very well positioned to play in those, let's call it, of very large scale systems. \\nHarsh Kumar: That's wonderful. And then maybe a quick one for Jean. So Jean, I put everything into the model that you talked about for June, I get about more or less a $400 million rise in the June quarter over March. You mentioned that both MI300 and EPYC will grow. Curious if you could help us think about the relative sizing of those 2 segments within the growth? I'm getting -- the point I'm trying to make is I'm getting roughly about a $900 million number for MI300 for June. Is that -- am I in the ballpark? Or am I way off here? \\nJean Hu: Harsh, we're not going to guide a specific segment below the segment revenue. I think the most important thing is that we did say Data Center is going to grow double digit sequentially. I will leave it over there. Subsegment, there are a lot of details. \\nOperator: There are no further questions at this time. I'd like to hand the floor back over to management for any closing comments. \\nMitchell Haws: Great. That concludes today's call. Thanks to all of you for joining us today. \\nLisa Su: Thanks. \\nOperator: This concludes today's conference. You may disconnect your lines at this time. Thank you for your participation.\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in subset:\n",
    "    file_name = '../data/'+ str(i['year']) + '_' + str(i['quarter']) + '_' + i['symbol']+'.txt'\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transcripts_2024.json', 'w') as f:\n",
    "    json.dump(new_data_ls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.json import JSONReader\n",
    "\n",
    "# Initialize JSONReader\n",
    "reader = JSONReader(\n",
    "    # The number of levels to go back in the JSON tree. Set to 0 to traverse all levels. Default is None.\n",
    "    levels_back=0,\n",
    "    # The maximum number of characters a JSON fragment would be collapsed in the output. Default is None.\n",
    "    collapse_length=None,\n",
    "    # If True, ensures that the output is ASCII-encoded. Default is False.\n",
    "    ensure_ascii=True,\n",
    "    # If True, indicates that the file is in JSONL (JSON Lines) format. Default is False.\n",
    "    is_jsonl=True,\n",
    "    # If True, removes lines containing only formatting from the output. Default is True.\n",
    "    clean_json=True,\n",
    ")\n",
    "\n",
    "# Load data from JSON file\n",
    "documents = reader.load_data(input_file=\"transcripts_2024.json\", extra_info={})\n",
    "documents = documents[:6]\n",
    "#docs to nodesj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /Users/michieldekoninck/Library/Caches/llama_index/models/Meta-Llama-3-8B-Instruct.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = dl\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
      "llm_load_print_meta: general.name     = dl\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4156.02 MiB, ( 4156.08 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4156.00 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 7008\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   876.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  876.00 MiB, K (f16):  438.00 MiB, V (f16):  438.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   483.69 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.69 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'dl', 'llama.vocab_size': '128256'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from transcripts_rag.local_llama import load_model\n",
    "\n",
    "llm = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "# embed_model = CohereEmbeddings(model=\"embed-english-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.json import JSONReader\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "\n",
    "transformations = [\n",
    "    #JSONReader(levels_back=0, collapse_length=None,ensure_ascii=True,  is_jsonl=True,clean_json=True ),\n",
    "    SentenceSplitter(),\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    #QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    #SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    KeywordExtractor(keywords=5, llm=llm),\n",
    "    EntityExtractor(prediction_threshold=0.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('transcripts_2024.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "documents = documents[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "\n",
    "async def run_pipeline(documents):\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.90 ms /   256 runs   (    0.06 ms per token, 17182.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8578.71 ms /  1100 tokens (    7.80 ms per token,   128.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9341.07 ms /   255 runs   (   36.63 ms per token,    27.30 tokens per second)\n",
      "llama_print_timings:       total time =   18134.93 ms /  1355 tokens\n",
      "Llama.generate: 62 prefix-match hit, remaining 1010 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.88 ms /   256 runs   (    0.06 ms per token, 16116.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4049.28 ms /  1010 tokens (    4.01 ms per token,   249.43 tokens per second)\n",
      "llama_print_timings:        eval time =    9441.77 ms /   255 runs   (   37.03 ms per token,    27.01 tokens per second)\n",
      "llama_print_timings:       total time =   13783.32 ms /  1265 tokens\n",
      "Llama.generate: 61 prefix-match hit, remaining 1006 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.82 ms /   256 runs   (    0.06 ms per token, 16184.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.01 ms /  1006 tokens (    4.02 ms per token,   248.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.98 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13795.30 ms /  1261 tokens\n",
      "Llama.generate: 61 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.38 ms /   256 runs   (    0.06 ms per token, 16647.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4162.30 ms /  1044 tokens (    3.99 ms per token,   250.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9337.75 ms /   255 runs   (   36.62 ms per token,    27.31 tokens per second)\n",
      "llama_print_timings:       total time =   13726.21 ms /  1299 tokens\n",
      "Llama.generate: 61 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.73 ms /   256 runs   (    0.06 ms per token, 16274.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.77 ms /  1052 tokens (    3.98 ms per token,   251.27 tokens per second)\n",
      "llama_print_timings:        eval time =    9327.28 ms /   255 runs   (   36.58 ms per token,    27.34 tokens per second)\n",
      "llama_print_timings:       total time =   13798.13 ms /  1307 tokens\n",
      "100%|██████████| 5/5 [01:13<00:00, 14.68s/it]\n",
      "Llama.generate: 59 prefix-match hit, remaining 1306 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.09 ms /   256 runs   (    0.06 ms per token, 16962.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5144.69 ms /  1306 tokens (    3.94 ms per token,   253.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9629.52 ms /   255 runs   (   37.76 ms per token,    26.48 tokens per second)\n",
      "llama_print_timings:       total time =   14974.17 ms /  1561 tokens\n",
      "  0%|          | 0/251 [00:00<?, ?it/s]Llama.generate: 60 prefix-match hit, remaining 1326 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.49 ms /   256 runs   (    0.06 ms per token, 16531.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5314.47 ms /  1326 tokens (    4.01 ms per token,   249.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9670.39 ms /   255 runs   (   37.92 ms per token,    26.37 tokens per second)\n",
      "llama_print_timings:       total time =   15210.89 ms /  1581 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1048 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.64 ms /   256 runs   (    0.06 ms per token, 16365.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4240.49 ms /  1048 tokens (    4.05 ms per token,   247.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9639.52 ms /   255 runs   (   37.80 ms per token,    26.45 tokens per second)\n",
      "llama_print_timings:       total time =   14127.29 ms /  1303 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 993 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.13 ms /   256 runs   (    0.06 ms per token, 16924.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4045.05 ms /   993 tokens (    4.07 ms per token,   245.49 tokens per second)\n",
      "llama_print_timings:        eval time =    9590.23 ms /   255 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
      "llama_print_timings:       total time =   13920.12 ms /  1248 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.36 ms /   256 runs   (    0.06 ms per token, 15647.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.86 ms /  1040 tokens (    4.05 ms per token,   247.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9631.93 ms /   255 runs   (   37.77 ms per token,    26.47 tokens per second)\n",
      "llama_print_timings:       total time =   14091.46 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1055 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.79 ms /   256 runs   (    0.06 ms per token, 16217.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4228.45 ms /  1055 tokens (    4.01 ms per token,   249.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9644.48 ms /   255 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
      "llama_print_timings:       total time =   14084.18 ms /  1310 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.80 ms /   256 runs   (    0.06 ms per token, 17301.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4207.42 ms /  1037 tokens (    4.06 ms per token,   246.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9605.35 ms /   255 runs   (   37.67 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =   14037.54 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 994 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.80 ms /   256 runs   (    0.06 ms per token, 16199.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.68 ms /   994 tokens (    4.07 ms per token,   245.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9590.43 ms /   255 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
      "llama_print_timings:       total time =   13933.28 ms /  1249 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /   256 runs   (    0.06 ms per token, 17106.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.84 ms /  1043 tokens (    4.04 ms per token,   247.64 tokens per second)\n",
      "llama_print_timings:        eval time =    9638.57 ms /   255 runs   (   37.80 ms per token,    26.46 tokens per second)\n",
      "llama_print_timings:       total time =   14094.38 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.33 ms /   256 runs   (    0.06 ms per token, 16702.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4228.98 ms /  1054 tokens (    4.01 ms per token,   249.23 tokens per second)\n",
      "llama_print_timings:        eval time =    9619.46 ms /   255 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14062.13 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.51 ms /   256 runs   (    0.06 ms per token, 16501.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4204.59 ms /  1050 tokens (    4.00 ms per token,   249.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9619.97 ms /   255 runs   (   37.73 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14048.34 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1028 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.42 ms /   256 runs   (    0.06 ms per token, 16598.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4191.78 ms /  1028 tokens (    4.08 ms per token,   245.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9594.42 ms /   255 runs   (   37.63 ms per token,    26.58 tokens per second)\n",
      "llama_print_timings:       total time =   14063.64 ms /  1283 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.20 ms /   256 runs   (    0.06 ms per token, 16838.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.04 ms /  1050 tokens (    4.01 ms per token,   249.11 tokens per second)\n",
      "llama_print_timings:        eval time =    9642.03 ms /   255 runs   (   37.81 ms per token,    26.45 tokens per second)\n",
      "llama_print_timings:       total time =   14097.80 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.93 ms /   256 runs   (    0.06 ms per token, 16068.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4201.10 ms /  1047 tokens (    4.01 ms per token,   249.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9649.80 ms /   255 runs   (   37.84 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14117.05 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.64 ms /   256 runs   (    0.07 ms per token, 15383.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4196.82 ms /  1046 tokens (    4.01 ms per token,   249.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9703.17 ms /   255 runs   (   38.05 ms per token,    26.28 tokens per second)\n",
      "llama_print_timings:       total time =   14192.32 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.02 ms /   256 runs   (    0.06 ms per token, 15985.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4214.00 ms /  1037 tokens (    4.06 ms per token,   246.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9619.06 ms /   255 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14076.80 ms /  1292 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1048 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.68 ms /   256 runs   (    0.06 ms per token, 16323.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.23 ms /  1048 tokens (    4.02 ms per token,   249.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9636.75 ms /   255 runs   (   37.79 ms per token,    26.46 tokens per second)\n",
      "llama_print_timings:       total time =   14113.44 ms /  1303 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /   256 runs   (    0.06 ms per token, 17110.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4223.72 ms /  1038 tokens (    4.07 ms per token,   245.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9604.46 ms /   255 runs   (   37.66 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =   14069.52 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1059 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.83 ms /   256 runs   (    0.06 ms per token, 16168.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4320.97 ms /  1059 tokens (    4.08 ms per token,   245.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9677.61 ms /   255 runs   (   37.95 ms per token,    26.35 tokens per second)\n",
      "llama_print_timings:       total time =   14227.88 ms /  1314 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.79 ms /   256 runs   (    0.06 ms per token, 16209.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4205.39 ms /  1033 tokens (    4.07 ms per token,   245.64 tokens per second)\n",
      "llama_print_timings:        eval time =    9617.88 ms /   255 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14079.89 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.59 ms /   256 runs   (    0.06 ms per token, 16417.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.16 ms /  1051 tokens (    4.01 ms per token,   249.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9645.74 ms /   255 runs   (   37.83 ms per token,    26.44 tokens per second)\n",
      "llama_print_timings:       total time =   14080.07 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.04 ms /   256 runs   (    0.06 ms per token, 15958.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4223.35 ms /  1040 tokens (    4.06 ms per token,   246.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9624.21 ms /   255 runs   (   37.74 ms per token,    26.50 tokens per second)\n",
      "llama_print_timings:       total time =   14106.64 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.29 ms /   256 runs   (    0.06 ms per token, 15719.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4218.89 ms /  1053 tokens (    4.01 ms per token,   249.59 tokens per second)\n",
      "llama_print_timings:        eval time =    9649.52 ms /   255 runs   (   37.84 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14156.66 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.82 ms /   256 runs   (    0.06 ms per token, 16181.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4209.15 ms /  1027 tokens (    4.10 ms per token,   243.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9610.03 ms /   255 runs   (   37.69 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14059.25 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      17.20 ms /   256 runs   (    0.07 ms per token, 14885.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4199.78 ms /  1029 tokens (    4.08 ms per token,   245.01 tokens per second)\n",
      "llama_print_timings:        eval time =    9647.64 ms /   255 runs   (   37.83 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14166.13 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1003 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.81 ms /   256 runs   (    0.06 ms per token, 16187.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4123.43 ms /  1003 tokens (    4.11 ms per token,   243.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9598.99 ms /   255 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
      "llama_print_timings:       total time =   13963.34 ms /  1258 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.61 ms /   256 runs   (    0.06 ms per token, 16403.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4219.21 ms /  1049 tokens (    4.02 ms per token,   248.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9635.88 ms /   255 runs   (   37.79 ms per token,    26.46 tokens per second)\n",
      "llama_print_timings:       total time =   14067.57 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.05 ms /   256 runs   (    0.06 ms per token, 17009.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4202.47 ms /  1026 tokens (    4.10 ms per token,   244.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9609.35 ms /   255 runs   (   37.68 ms per token,    26.54 tokens per second)\n",
      "llama_print_timings:       total time =   14042.84 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.87 ms /   256 runs   (    0.06 ms per token, 16127.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1245.90 ms /   312 tokens (    3.99 ms per token,   250.42 tokens per second)\n",
      "llama_print_timings:        eval time =    8711.62 ms /   255 runs   (   34.16 ms per token,    29.27 tokens per second)\n",
      "llama_print_timings:       total time =   10170.45 ms /   567 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1032 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.57 ms /   256 runs   (    0.06 ms per token, 16443.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4212.70 ms /  1032 tokens (    4.08 ms per token,   244.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9611.21 ms /   255 runs   (   37.69 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14085.01 ms /  1287 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.91 ms /   256 runs   (    0.06 ms per token, 17171.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4230.76 ms /  1040 tokens (    4.07 ms per token,   245.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9623.80 ms /   255 runs   (   37.74 ms per token,    26.50 tokens per second)\n",
      "llama_print_timings:       total time =   14089.06 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.58 ms /   256 runs   (    0.06 ms per token, 16433.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.77 ms /  1049 tokens (    4.02 ms per token,   248.83 tokens per second)\n",
      "llama_print_timings:        eval time =    9622.48 ms /   255 runs   (   37.74 ms per token,    26.50 tokens per second)\n",
      "llama_print_timings:       total time =   14052.94 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.97 ms /   256 runs   (    0.06 ms per token, 16033.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.77 ms /  1042 tokens (    4.05 ms per token,   247.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9626.33 ms /   255 runs   (   37.75 ms per token,    26.49 tokens per second)\n",
      "llama_print_timings:       total time =   14072.39 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.77 ms /   256 runs   (    0.06 ms per token, 16230.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4206.04 ms /  1051 tokens (    4.00 ms per token,   249.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9630.83 ms /   255 runs   (   37.77 ms per token,    26.48 tokens per second)\n",
      "llama_print_timings:       total time =   14086.99 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1002 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.63 ms /   256 runs   (    0.06 ms per token, 16380.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.58 ms /  1002 tokens (    4.03 ms per token,   248.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9595.72 ms /   255 runs   (   37.63 ms per token,    26.57 tokens per second)\n",
      "llama_print_timings:       total time =   13895.80 ms /  1257 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.39 ms /   256 runs   (    0.06 ms per token, 16636.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4201.66 ms /  1038 tokens (    4.05 ms per token,   247.05 tokens per second)\n",
      "llama_print_timings:        eval time =    9605.23 ms /   255 runs   (   37.67 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =   14012.50 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.22 ms /   256 runs   (    0.06 ms per token, 15786.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4193.40 ms /  1035 tokens (    4.05 ms per token,   246.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9612.50 ms /   255 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14045.39 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.21 ms /   256 runs   (    0.06 ms per token, 16836.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4207.52 ms /  1027 tokens (    4.10 ms per token,   244.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9625.81 ms /   255 runs   (   37.75 ms per token,    26.49 tokens per second)\n",
      "llama_print_timings:       total time =   14140.21 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 988 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.75 ms /   256 runs   (    0.06 ms per token, 16253.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3947.08 ms /   988 tokens (    4.00 ms per token,   250.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9569.63 ms /   255 runs   (   37.53 ms per token,    26.65 tokens per second)\n",
      "llama_print_timings:       total time =   13758.23 ms /  1243 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.87 ms /   256 runs   (    0.06 ms per token, 16134.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.96 ms /  1045 tokens (    4.03 ms per token,   248.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9613.30 ms /   255 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14043.51 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.00 ms /   256 runs   (    0.06 ms per token, 17065.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.03 ms /  1053 tokens (    4.00 ms per token,   250.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9633.85 ms /   255 runs   (   37.78 ms per token,    26.47 tokens per second)\n",
      "llama_print_timings:       total time =   14063.77 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.08 ms /   256 runs   (    0.06 ms per token, 15918.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4222.60 ms /  1030 tokens (    4.10 ms per token,   243.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9531.78 ms /   255 runs   (   37.38 ms per token,    26.75 tokens per second)\n",
      "llama_print_timings:       total time =   14002.21 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18721.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.19 ms /  1042 tokens (    4.02 ms per token,   248.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9507.64 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13883.58 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.81 ms /   256 runs   (    0.05 ms per token, 18534.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.97 ms /  1051 tokens (    3.99 ms per token,   250.54 tokens per second)\n",
      "llama_print_timings:        eval time =    9497.11 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13857.25 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.77 ms /   256 runs   (    0.05 ms per token, 18595.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4171.67 ms /  1030 tokens (    4.05 ms per token,   246.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9477.84 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13848.98 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.77 ms /   256 runs   (    0.05 ms per token, 18591.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.25 ms /  1038 tokens (    4.03 ms per token,   248.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9480.64 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13828.51 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18304.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4175.18 ms /  1038 tokens (    4.02 ms per token,   248.61 tokens per second)\n",
      "llama_print_timings:        eval time =    9517.44 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13930.99 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /   256 runs   (    0.05 ms per token, 18905.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.33 ms /  1050 tokens (    3.99 ms per token,   250.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.22 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13893.31 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.25 ms /   256 runs   (    0.06 ms per token, 17963.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4189.58 ms /  1046 tokens (    4.01 ms per token,   249.67 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.95 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13895.59 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1032 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.09 ms /   256 runs   (    0.06 ms per token, 18163.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.96 ms /  1032 tokens (    4.05 ms per token,   247.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9566.37 ms /   255 runs   (   37.52 ms per token,    26.66 tokens per second)\n",
      "llama_print_timings:       total time =   14087.47 ms /  1287 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18797.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.31 ms /  1042 tokens (    4.04 ms per token,   247.43 tokens per second)\n",
      "llama_print_timings:        eval time =    9493.66 ms /   255 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13932.25 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1065 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.85 ms /   256 runs   (    0.05 ms per token, 18490.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.97 ms /  1065 tokens (    4.04 ms per token,   247.68 tokens per second)\n",
      "llama_print_timings:        eval time =    9557.42 ms /   255 runs   (   37.48 ms per token,    26.68 tokens per second)\n",
      "llama_print_timings:       total time =   14046.63 ms /  1320 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1071 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /   256 runs   (    0.05 ms per token, 18898.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4303.55 ms /  1071 tokens (    4.02 ms per token,   248.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9520.38 ms /   255 runs   (   37.33 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13993.72 ms /  1326 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18294.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4043.94 ms /  1022 tokens (    3.96 ms per token,   252.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9457.07 ms /   255 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13672.90 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1060 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18668.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4296.16 ms /  1060 tokens (    4.05 ms per token,   246.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9538.01 ms /   255 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   14032.58 ms /  1315 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18466.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.16 ms /  1041 tokens (    4.03 ms per token,   248.20 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.64 ms /   255 runs   (   37.29 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13898.29 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.35 ms /   256 runs   (    0.06 ms per token, 17837.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.06 ms /  1029 tokens (    4.06 ms per token,   246.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9524.30 ms /   255 runs   (   37.35 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13962.29 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1067 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18798.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4333.76 ms /  1067 tokens (    4.06 ms per token,   246.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9535.50 ms /   255 runs   (   37.39 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   14060.17 ms /  1322 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18855.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.61 ms /  1050 tokens (    3.99 ms per token,   250.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.99 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13876.28 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18721.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.88 ms /  1039 tokens (    4.03 ms per token,   248.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.91 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13902.09 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /   256 runs   (    0.05 ms per token, 18422.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4304.19 ms /  1066 tokens (    4.04 ms per token,   247.67 tokens per second)\n",
      "llama_print_timings:        eval time =    9526.96 ms /   255 runs   (   37.36 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   14030.23 ms /  1321 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.78 ms /   256 runs   (    0.05 ms per token, 18581.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.70 ms /  1036 tokens (    4.04 ms per token,   247.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9466.69 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13813.64 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1002 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18652.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4027.42 ms /  1002 tokens (    4.02 ms per token,   248.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9429.43 ms /   255 runs   (   36.98 ms per token,    27.04 tokens per second)\n",
      "llama_print_timings:       total time =   13634.19 ms /  1257 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18475.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.55 ms /  1053 tokens (    3.97 ms per token,   251.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9526.77 ms /   255 runs   (   37.36 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13911.53 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18388.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.90 ms /  1041 tokens (    4.02 ms per token,   248.63 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.55 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13868.29 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.94 ms /   256 runs   (    0.05 ms per token, 18368.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.95 ms /  1039 tokens (    4.02 ms per token,   248.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.07 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13889.61 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18669.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4303.60 ms /  1066 tokens (    4.04 ms per token,   247.70 tokens per second)\n",
      "llama_print_timings:        eval time =    9515.39 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   14007.42 ms /  1321 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1057 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18566.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4305.86 ms /  1057 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9521.62 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   14017.90 ms /  1312 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /   256 runs   (    0.05 ms per token, 18607.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.49 ms /  1037 tokens (    4.04 ms per token,   247.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.27 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13915.70 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.60 ms /   256 runs   (    0.05 ms per token, 18820.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.26 ms /  1034 tokens (    4.04 ms per token,   247.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.71 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13823.62 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1056 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.49 ms /   256 runs   (    0.05 ms per token, 18979.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.28 ms /  1056 tokens (    3.96 ms per token,   252.37 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.10 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13870.20 ms /  1311 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.03 ms /   256 runs   (    0.05 ms per token, 18247.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.60 ms /  1039 tokens (    4.02 ms per token,   248.59 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.06 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13844.77 ms /  1294 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18961.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.06 ms /  1023 tokens (    3.94 ms per token,   253.78 tokens per second)\n",
      "llama_print_timings:        eval time =    9464.35 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13680.38 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.39 ms /   256 runs   (    0.05 ms per token, 19117.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.03 ms /  1037 tokens (    4.03 ms per token,   247.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9459.85 ms /   255 runs   (   37.10 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13794.85 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18870.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.51 ms /  1040 tokens (    4.02 ms per token,   248.95 tokens per second)\n",
      "llama_print_timings:        eval time =    9495.66 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13861.12 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18867.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4038.04 ms /  1022 tokens (    3.95 ms per token,   253.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9449.48 ms /   255 runs   (   37.06 ms per token,    26.99 tokens per second)\n",
      "llama_print_timings:       total time =   13653.48 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.19 ms /   256 runs   (    0.06 ms per token, 18043.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.03 ms /  1050 tokens (    3.98 ms per token,   251.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9515.69 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13954.71 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18860.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.61 ms /  1040 tokens (    4.02 ms per token,   248.65 tokens per second)\n",
      "llama_print_timings:        eval time =    9508.75 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13880.52 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1061 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18615.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4303.98 ms /  1061 tokens (    4.06 ms per token,   246.52 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.35 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   14000.69 ms /  1316 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.03 ms /   256 runs   (    0.05 ms per token, 18241.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.89 ms /  1043 tokens (    4.01 ms per token,   249.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9503.28 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13877.56 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.96 ms /   256 runs   (    0.05 ms per token, 18342.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.92 ms /  1034 tokens (    4.04 ms per token,   247.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.03 ms /   255 runs   (   37.21 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13865.56 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18815.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.51 ms /  1039 tokens (    4.03 ms per token,   248.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.99 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13870.14 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18996.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4032.80 ms /  1020 tokens (    3.95 ms per token,   252.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9461.57 ms /   255 runs   (   37.10 ms per token,    26.95 tokens per second)\n",
      "llama_print_timings:       total time =   13681.01 ms /  1275 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /   256 runs   (    0.05 ms per token, 18782.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.12 ms /  1038 tokens (    4.03 ms per token,   248.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9472.66 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13815.14 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1031 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18958.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.91 ms /  1031 tokens (    4.05 ms per token,   246.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9476.11 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13831.28 ms /  1286 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1056 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.89 ms /   256 runs   (    0.05 ms per token, 18425.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.31 ms /  1056 tokens (    3.95 ms per token,   253.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9507.57 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13872.87 ms /  1311 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18831.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.48 ms /  1026 tokens (    4.08 ms per token,   244.84 tokens per second)\n",
      "llama_print_timings:        eval time =    9474.63 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13852.50 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18809.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.71 ms /  1049 tokens (    3.99 ms per token,   250.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9490.84 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13844.24 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1062 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.52 ms /   256 runs   (    0.05 ms per token, 18936.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4302.71 ms /  1062 tokens (    4.05 ms per token,   246.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9513.82 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   14002.07 ms /  1317 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1005 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.11 ms /   256 runs   (    0.06 ms per token, 18141.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4027.08 ms /  1005 tokens (    4.01 ms per token,   249.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.66 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13699.11 ms /  1260 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /   256 runs   (    0.05 ms per token, 18913.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.15 ms /  1044 tokens (    4.01 ms per token,   249.39 tokens per second)\n",
      "llama_print_timings:        eval time =    9495.74 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13857.88 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.87 ms /   256 runs   (    0.05 ms per token, 18455.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.50 ms /  1029 tokens (    4.06 ms per token,   246.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.43 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13854.83 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1011 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18883.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.66 ms /  1011 tokens (    3.99 ms per token,   250.77 tokens per second)\n",
      "llama_print_timings:        eval time =    9444.07 ms /   255 runs   (   37.04 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13653.04 ms /  1266 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18394.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.22 ms /  1054 tokens (    3.97 ms per token,   251.96 tokens per second)\n",
      "llama_print_timings:        eval time =    9524.68 ms /   255 runs   (   37.35 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13932.81 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.40 ms /   256 runs   (    0.06 ms per token, 17779.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4172.15 ms /  1030 tokens (    4.05 ms per token,   246.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9510.66 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13907.13 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18852.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.99 ms /  1043 tokens (    4.02 ms per token,   248.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.21 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13861.88 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18986.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.44 ms /  1053 tokens (    3.97 ms per token,   251.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.15 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13830.01 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18699.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.80 ms /  1040 tokens (    4.03 ms per token,   248.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.62 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13840.03 ms /  1295 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18962.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.20 ms /  1044 tokens (    4.00 ms per token,   249.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9497.56 ms /   255 runs   (   37.25 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13880.18 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1055 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /   256 runs   (    0.05 ms per token, 18897.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.97 ms /  1055 tokens (    3.97 ms per token,   252.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9495.55 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13843.26 ms /  1310 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18215.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4258.06 ms /  1047 tokens (    4.07 ms per token,   245.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.09 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13938.63 ms /  1302 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /   256 runs   (    0.05 ms per token, 18606.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.79 ms /  1036 tokens (    4.03 ms per token,   247.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9478.26 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13828.76 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1067 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.64 ms /   256 runs   (    0.05 ms per token, 18761.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4298.54 ms /  1067 tokens (    4.03 ms per token,   248.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9548.35 ms /   255 runs   (   37.44 ms per token,    26.71 tokens per second)\n",
      "llama_print_timings:       total time =   14051.85 ms /  1322 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1048 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /   256 runs   (    0.05 ms per token, 18784.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.23 ms /  1048 tokens (    3.99 ms per token,   250.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9522.61 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13912.09 ms /  1303 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.73 ms /   256 runs   (    0.05 ms per token, 18646.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.69 ms /  1035 tokens (    4.03 ms per token,   247.92 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.34 ms /   255 runs   (   37.22 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13836.46 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1070 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18791.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4304.82 ms /  1070 tokens (    4.02 ms per token,   248.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9511.97 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13983.81 ms /  1325 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1060 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.94 ms /   256 runs   (    0.05 ms per token, 18360.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4300.52 ms /  1060 tokens (    4.06 ms per token,   246.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9547.33 ms /   255 runs   (   37.44 ms per token,    26.71 tokens per second)\n",
      "llama_print_timings:       total time =   14082.08 ms /  1315 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.24 ms /   256 runs   (    0.06 ms per token, 17977.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.83 ms /  1026 tokens (    4.07 ms per token,   245.64 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.77 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13888.66 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18392.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4188.26 ms /  1051 tokens (    3.99 ms per token,   250.94 tokens per second)\n",
      "llama_print_timings:        eval time =    9517.79 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13906.50 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.77 ms /   256 runs   (    0.05 ms per token, 18593.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.45 ms /  1046 tokens (    4.00 ms per token,   249.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.94 ms /   255 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13851.84 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.94 ms /   256 runs   (    0.05 ms per token, 18367.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.90 ms /  1020 tokens (    3.96 ms per token,   252.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9464.58 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13676.00 ms /  1275 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18809.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.25 ms /  1049 tokens (    4.00 ms per token,   250.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.51 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13909.58 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18958.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4227.80 ms /  1050 tokens (    4.03 ms per token,   248.36 tokens per second)\n",
      "llama_print_timings:        eval time =    9507.55 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13923.04 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18548.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.35 ms /  1042 tokens (    4.02 ms per token,   248.55 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.48 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13927.28 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1056 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18754.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.68 ms /  1056 tokens (    3.96 ms per token,   252.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.50 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13843.22 ms /  1311 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      17.45 ms /   256 runs   (    0.07 ms per token, 14671.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4057.51 ms /  1022 tokens (    3.97 ms per token,   251.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9477.18 ms /   255 runs   (   37.17 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13752.94 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1002 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18816.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4035.59 ms /  1002 tokens (    4.03 ms per token,   248.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9445.03 ms /   255 runs   (   37.04 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13662.22 ms /  1257 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.87 ms /   256 runs   (    0.05 ms per token, 18458.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.13 ms /  1045 tokens (    4.00 ms per token,   249.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.99 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13851.02 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1010 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18471.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.45 ms /  1010 tokens (    4.00 ms per token,   249.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9443.73 ms /   255 runs   (   37.03 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13672.71 ms /  1265 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18717.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.10 ms /  1039 tokens (    4.02 ms per token,   248.80 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.36 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13841.39 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18631.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.18 ms /  1042 tokens (    4.01 ms per token,   249.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9506.72 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13889.75 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18742.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.60 ms /  1034 tokens (    4.04 ms per token,   247.33 tokens per second)\n",
      "llama_print_timings:        eval time =    9466.57 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13818.19 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18223.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.62 ms /  1046 tokens (    4.00 ms per token,   250.02 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.70 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13898.89 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.60 ms /   256 runs   (    0.05 ms per token, 18829.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.39 ms /  1047 tokens (    4.00 ms per token,   250.28 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.14 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13865.10 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18656.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.43 ms /  1026 tokens (    4.08 ms per token,   245.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9502.52 ms /   255 runs   (   37.26 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13899.72 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18865.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4074.73 ms /  1023 tokens (    3.98 ms per token,   251.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9463.66 ms /   255 runs   (   37.11 ms per token,    26.95 tokens per second)\n",
      "llama_print_timings:       total time =   13724.91 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18223.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4310.48 ms /  1058 tokens (    4.07 ms per token,   245.45 tokens per second)\n",
      "llama_print_timings:        eval time =    9520.74 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   14009.72 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 944 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18701.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3778.02 ms /   944 tokens (    4.00 ms per token,   249.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9360.10 ms /   255 runs   (   36.71 ms per token,    27.24 tokens per second)\n",
      "llama_print_timings:       total time =   13337.99 ms /  1199 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18662.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.69 ms /  1036 tokens (    4.04 ms per token,   247.57 tokens per second)\n",
      "llama_print_timings:        eval time =    9518.45 ms /   255 runs   (   37.33 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13905.94 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.95 ms /   256 runs   (    0.05 ms per token, 18352.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.53 ms /  1037 tokens (    4.03 ms per token,   247.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9502.77 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13868.09 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1003 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.11 ms /   256 runs   (    0.06 ms per token, 18149.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4021.79 ms /  1003 tokens (    4.01 ms per token,   249.39 tokens per second)\n",
      "llama_print_timings:        eval time =    9456.87 ms /   255 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13671.59 ms /  1258 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 962 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.09 ms /   256 runs   (    0.06 ms per token, 18163.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3897.89 ms /   962 tokens (    4.05 ms per token,   246.80 tokens per second)\n",
      "llama_print_timings:        eval time =    9403.56 ms /   255 runs   (   36.88 ms per token,    27.12 tokens per second)\n",
      "llama_print_timings:       total time =   13491.64 ms /  1217 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1073 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.12 ms /   256 runs   (    0.06 ms per token, 18132.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4333.98 ms /  1073 tokens (    4.04 ms per token,   247.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.45 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   14084.75 ms /  1328 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.00 ms /   256 runs   (    0.05 ms per token, 18287.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.92 ms /  1030 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.28 ms /   255 runs   (   37.21 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13865.39 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1008 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.52 ms /   256 runs   (    0.05 ms per token, 18930.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4036.51 ms /  1008 tokens (    4.00 ms per token,   249.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9432.53 ms /   255 runs   (   36.99 ms per token,    27.03 tokens per second)\n",
      "llama_print_timings:       total time =   13617.70 ms /  1263 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1065 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18754.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4307.26 ms /  1065 tokens (    4.04 ms per token,   247.26 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.78 ms /   255 runs   (   37.31 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13979.03 ms /  1320 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18869.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.86 ms /  1052 tokens (    3.98 ms per token,   251.26 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.18 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13809.04 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /   256 runs   (    0.05 ms per token, 18414.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.79 ms /  1026 tokens (    4.07 ms per token,   245.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.58 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13819.57 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1064 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18849.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4316.49 ms /  1064 tokens (    4.06 ms per token,   246.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9504.36 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13983.58 ms /  1319 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.46 ms /   256 runs   (    0.05 ms per token, 19017.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.22 ms /  1042 tokens (    4.03 ms per token,   248.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9478.49 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13824.29 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18859.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.91 ms /  1035 tokens (    4.03 ms per token,   247.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.36 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13769.29 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1067 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.84 ms /   256 runs   (    0.05 ms per token, 18502.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4297.97 ms /  1067 tokens (    4.03 ms per token,   248.26 tokens per second)\n",
      "llama_print_timings:        eval time =    9537.65 ms /   255 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   14042.94 ms /  1322 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18790.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4046.55 ms /  1022 tokens (    3.96 ms per token,   252.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9472.50 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13697.90 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.94 ms /   256 runs   (    0.06 ms per token, 17137.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.09 ms /  1058 tokens (    4.06 ms per token,   246.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9589.28 ms /   255 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
      "llama_print_timings:       total time =   14082.27 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1071 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.37 ms /   256 runs   (    0.06 ms per token, 17819.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4302.12 ms /  1071 tokens (    4.02 ms per token,   248.95 tokens per second)\n",
      "llama_print_timings:        eval time =    9534.20 ms /   255 runs   (   37.39 ms per token,    26.75 tokens per second)\n",
      "llama_print_timings:       total time =   13991.12 ms /  1326 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1007 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.73 ms /   256 runs   (    0.05 ms per token, 18645.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4024.59 ms /  1007 tokens (    4.00 ms per token,   250.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.49 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13669.50 ms /  1262 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.82 ms /   256 runs   (    0.05 ms per token, 18526.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.45 ms /  1050 tokens (    4.01 ms per token,   249.32 tokens per second)\n",
      "llama_print_timings:        eval time =    9525.19 ms /   255 runs   (   37.35 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13933.99 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1025 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18995.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.18 ms /  1024 tokens (    3.94 ms per token,   254.02 tokens per second)\n",
      "llama_print_timings:        eval time =    9477.58 ms /   256 runs   (   37.02 ms per token,    27.01 tokens per second)\n",
      "llama_print_timings:       total time =   13658.83 ms /  1280 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18467.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.88 ms /  1044 tokens (    4.00 ms per token,   250.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.38 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13794.70 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18717.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.92 ms /  1036 tokens (    4.03 ms per token,   247.91 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.26 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13772.56 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18615.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.60 ms /  1037 tokens (    4.03 ms per token,   248.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9490.05 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13860.65 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1008 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.45 ms /   256 runs   (    0.05 ms per token, 19029.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4027.90 ms /  1008 tokens (    4.00 ms per token,   250.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9418.03 ms /   255 runs   (   36.93 ms per token,    27.08 tokens per second)\n",
      "llama_print_timings:       total time =   13618.01 ms /  1263 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18215.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.65 ms /  1041 tokens (    4.02 ms per token,   249.00 tokens per second)\n",
      "llama_print_timings:        eval time =    9528.49 ms /   255 runs   (   37.37 ms per token,    26.76 tokens per second)\n",
      "llama_print_timings:       total time =   13898.87 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18548.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.82 ms /  1039 tokens (    4.03 ms per token,   248.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.94 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13837.49 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1009 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18566.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4034.88 ms /  1009 tokens (    4.00 ms per token,   250.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9446.98 ms /   255 runs   (   37.05 ms per token,    26.99 tokens per second)\n",
      "llama_print_timings:       total time =   13665.70 ms /  1264 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1059 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18612.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.80 ms /  1059 tokens (    4.06 ms per token,   246.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.72 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13986.85 ms /  1314 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18790.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.29 ms /  1033 tokens (    4.04 ms per token,   247.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.89 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13841.00 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18841.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.80 ms /  1038 tokens (    4.02 ms per token,   248.52 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.50 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13813.06 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1004 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.49 ms /   256 runs   (    0.05 ms per token, 18981.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4024.85 ms /  1004 tokens (    4.01 ms per token,   249.45 tokens per second)\n",
      "llama_print_timings:        eval time =    9423.35 ms /   255 runs   (   36.95 ms per token,    27.06 tokens per second)\n",
      "llama_print_timings:       total time =   13593.61 ms /  1259 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   256 runs   (    0.05 ms per token, 18380.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4023.37 ms /  1024 tokens (    3.93 ms per token,   254.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.67 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13701.04 ms /  1279 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.89 ms /   256 runs   (    0.05 ms per token, 18431.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.13 ms /  1037 tokens (    4.04 ms per token,   247.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.15 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13843.00 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.51 ms /   256 runs   (    0.05 ms per token, 18955.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.09 ms /  1035 tokens (    4.04 ms per token,   247.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9474.11 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13826.35 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.46 ms /   256 runs   (    0.05 ms per token, 19013.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.51 ms /  1041 tokens (    4.03 ms per token,   248.12 tokens per second)\n",
      "llama_print_timings:        eval time =    9478.15 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13839.97 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.88 ms /   256 runs   (    0.05 ms per token, 18445.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4193.85 ms /  1036 tokens (    4.05 ms per token,   247.03 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.90 ms /   255 runs   (   37.31 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13913.10 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18736.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.15 ms /  1052 tokens (    4.00 ms per token,   249.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.70 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13923.82 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1018 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18838.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4045.95 ms /  1018 tokens (    3.97 ms per token,   251.61 tokens per second)\n",
      "llama_print_timings:        eval time =    9454.32 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13671.92 ms /  1273 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /   256 runs   (    0.05 ms per token, 18783.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.23 ms /  1045 tokens (    4.01 ms per token,   249.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9504.88 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13897.75 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18561.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.70 ms /  1029 tokens (    4.08 ms per token,   245.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9508.20 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13918.22 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.84 ms /   256 runs   (    0.05 ms per token, 18494.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.78 ms /  1042 tokens (    4.02 ms per token,   249.00 tokens per second)\n",
      "llama_print_timings:        eval time =    9510.91 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13894.36 ms /  1297 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18842.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.88 ms /  1027 tokens (    4.07 ms per token,   245.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.55 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13839.43 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.43 ms /   256 runs   (    0.05 ms per token, 19063.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4300.33 ms /  1058 tokens (    4.06 ms per token,   246.03 tokens per second)\n",
      "llama_print_timings:        eval time =    9506.06 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   14003.66 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1019 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.96 ms /   256 runs   (    0.05 ms per token, 18343.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.81 ms /  1019 tokens (    3.97 ms per token,   251.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9480.84 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13713.04 ms /  1274 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18881.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.41 ms /  1029 tokens (    4.06 ms per token,   246.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9503.43 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13882.64 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.49 ms /   256 runs   (    0.05 ms per token, 18974.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.87 ms /  1029 tokens (    4.07 ms per token,   245.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.87 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13831.29 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18552.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.70 ms /  1047 tokens (    4.00 ms per token,   250.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.10 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13869.99 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1012 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18623.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4041.14 ms /  1012 tokens (    3.99 ms per token,   250.42 tokens per second)\n",
      "llama_print_timings:        eval time =    9443.59 ms /   255 runs   (   37.03 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13650.55 ms /  1267 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18738.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.53 ms /  1027 tokens (    4.07 ms per token,   245.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.21 ms /   255 runs   (   37.22 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13868.78 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1011 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18733.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4041.01 ms /  1011 tokens (    4.00 ms per token,   250.18 tokens per second)\n",
      "llama_print_timings:        eval time =    9442.51 ms /   255 runs   (   37.03 ms per token,    27.01 tokens per second)\n",
      "llama_print_timings:       total time =   13662.37 ms /  1266 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18469.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.25 ms /  1050 tokens (    3.98 ms per token,   251.36 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.09 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13877.49 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1018 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18673.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.48 ms /  1018 tokens (    3.97 ms per token,   252.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9457.72 ms /   255 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13691.53 ms /  1273 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1017 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.87 ms /   256 runs   (    0.06 ms per token, 17214.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.40 ms /  1017 tokens (    3.98 ms per token,   251.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.60 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13704.97 ms /  1272 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18852.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.93 ms /  1038 tokens (    4.03 ms per token,   248.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9497.21 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13854.91 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1055 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18626.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.58 ms /  1055 tokens (    3.97 ms per token,   251.63 tokens per second)\n",
      "llama_print_timings:        eval time =    9515.20 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13911.59 ms /  1310 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18725.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.38 ms /  1023 tokens (    3.95 ms per token,   253.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9451.91 ms /   255 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =   13661.55 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1019 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18849.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4050.01 ms /  1019 tokens (    3.97 ms per token,   251.60 tokens per second)\n",
      "llama_print_timings:        eval time =    9469.38 ms /   255 runs   (   37.13 ms per token,    26.93 tokens per second)\n",
      "llama_print_timings:       total time =   13691.09 ms /  1274 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.01 ms /   256 runs   (    0.05 ms per token, 18271.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.08 ms /  1044 tokens (    4.00 ms per token,   249.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9519.64 ms /   255 runs   (   37.33 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13902.39 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1057 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.83 ms /   256 runs   (    0.05 ms per token, 18517.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4296.85 ms /  1057 tokens (    4.07 ms per token,   245.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9527.49 ms /   255 runs   (   37.36 ms per token,    26.76 tokens per second)\n",
      "llama_print_timings:       total time =   14025.51 ms /  1312 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18705.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.45 ms /  1030 tokens (    4.05 ms per token,   246.74 tokens per second)\n",
      "llama_print_timings:        eval time =    9476.96 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13841.80 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18293.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.26 ms /  1034 tokens (    4.04 ms per token,   247.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.96 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13835.74 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   256 runs   (    0.05 ms per token, 18374.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4175.75 ms /  1040 tokens (    4.02 ms per token,   249.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9523.53 ms /   255 runs   (   37.35 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13911.86 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1025 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18716.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4046.55 ms /  1024 tokens (    3.95 ms per token,   253.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.28 ms /   256 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13759.56 ms /  1280 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /   256 runs   (    0.05 ms per token, 18912.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4189.79 ms /  1041 tokens (    4.02 ms per token,   248.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9487.16 ms /   255 runs   (   37.20 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13848.39 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1031 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18703.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4187.34 ms /  1031 tokens (    4.06 ms per token,   246.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9483.52 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13856.72 ms /  1286 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1015 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      20.11 ms /   256 runs   (    0.08 ms per token, 12729.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.11 ms /  1015 tokens (    3.98 ms per token,   251.42 tokens per second)\n",
      "llama_print_timings:        eval time =    9489.68 ms /   255 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13717.04 ms /  1270 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.51 ms /   256 runs   (    0.05 ms per token, 18943.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4196.94 ms /  1040 tokens (    4.04 ms per token,   247.80 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.43 ms /   255 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13885.87 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18697.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.16 ms /  1051 tokens (    3.98 ms per token,   251.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9518.86 ms /   255 runs   (   37.33 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13894.06 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1031 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.31 ms /   256 runs   (    0.06 ms per token, 17884.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4171.19 ms /  1031 tokens (    4.05 ms per token,   247.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.92 ms /   255 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13853.01 ms /  1286 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 999 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.01 ms /   256 runs   (    0.05 ms per token, 18275.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4021.12 ms /   999 tokens (    4.03 ms per token,   248.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.60 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13670.74 ms /  1254 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18664.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4187.50 ms /  1042 tokens (    4.02 ms per token,   248.84 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.21 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13901.06 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1014 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18995.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4070.63 ms /  1014 tokens (    4.01 ms per token,   249.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9454.83 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13712.56 ms /  1269 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18301.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.97 ms /  1052 tokens (    3.99 ms per token,   250.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9499.26 ms /   255 runs   (   37.25 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13864.26 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1013 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.73 ms /   256 runs   (    0.05 ms per token, 18638.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.87 ms /  1013 tokens (    3.99 ms per token,   250.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9451.69 ms /   255 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =   13674.93 ms /  1268 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18471.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.24 ms /  1047 tokens (    4.00 ms per token,   250.28 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.30 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13893.85 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1021 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18389.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4065.02 ms /  1021 tokens (    3.98 ms per token,   251.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.79 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13699.19 ms /  1276 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1032 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18747.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.85 ms /  1032 tokens (    4.05 ms per token,   247.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9498.64 ms /   255 runs   (   37.25 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13874.72 ms /  1287 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.41 ms /   256 runs   (    0.05 ms per token, 19093.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.83 ms /  1045 tokens (    4.00 ms per token,   249.83 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.20 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13835.86 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18470.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.22 ms /  1054 tokens (    3.97 ms per token,   251.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9537.48 ms /   255 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   13940.54 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1018 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.00 ms /   256 runs   (    0.05 ms per token, 18283.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4038.38 ms /  1018 tokens (    3.97 ms per token,   252.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.95 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13762.00 ms /  1273 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.90 ms /   256 runs   (    0.06 ms per token, 16100.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.57 ms /  1045 tokens (    4.00 ms per token,   249.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.38 ms /   255 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13872.15 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 993 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.14 ms /   256 runs   (    0.06 ms per token, 18100.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4015.37 ms /   993 tokens (    4.04 ms per token,   247.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9454.61 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13662.68 ms /  1248 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.97 ms /   256 runs   (    0.05 ms per token, 18322.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.28 ms /  1024 tokens (    3.94 ms per token,   254.01 tokens per second)\n",
      "llama_print_timings:        eval time =    9481.15 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13687.84 ms /  1279 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18658.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4199.18 ms /  1039 tokens (    4.04 ms per token,   247.43 tokens per second)\n",
      "llama_print_timings:        eval time =    9528.39 ms /   255 runs   (   37.37 ms per token,    26.76 tokens per second)\n",
      "llama_print_timings:       total time =   13937.29 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18618.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.29 ms /  1037 tokens (    4.03 ms per token,   248.01 tokens per second)\n",
      "llama_print_timings:        eval time =    9483.17 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13851.92 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1017 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18545.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4033.99 ms /  1017 tokens (    3.97 ms per token,   252.11 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.46 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13690.37 ms /  1272 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.78 ms /   256 runs   (    0.05 ms per token, 18574.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.58 ms /  1052 tokens (    3.97 ms per token,   251.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.62 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13836.47 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.53 ms /   256 runs   (    0.05 ms per token, 18918.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.63 ms /  1044 tokens (    4.01 ms per token,   249.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9498.19 ms /   255 runs   (   37.25 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13875.96 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18705.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.07 ms /  1039 tokens (    4.03 ms per token,   248.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9470.74 ms /   255 runs   (   37.14 ms per token,    26.93 tokens per second)\n",
      "llama_print_timings:       total time =   13830.10 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.06 ms /   256 runs   (    0.05 ms per token, 18206.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.55 ms /  1043 tokens (    4.01 ms per token,   249.67 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.52 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13852.25 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.84 ms /   256 runs   (    0.05 ms per token, 18501.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4305.60 ms /  1058 tokens (    4.07 ms per token,   245.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.22 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13963.39 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1069 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18753.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4309.29 ms /  1069 tokens (    4.03 ms per token,   248.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9533.36 ms /   255 runs   (   37.39 ms per token,    26.75 tokens per second)\n",
      "llama_print_timings:       total time =   14007.90 ms /  1324 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18876.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.27 ms /  1043 tokens (    4.02 ms per token,   248.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.32 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13860.13 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.55 ms /   256 runs   (    0.06 ms per token, 17588.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.19 ms /  1040 tokens (    4.02 ms per token,   248.55 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.97 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13888.33 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18550.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.50 ms /  1045 tokens (    4.01 ms per token,   249.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9514.93 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13923.99 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1065 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.83 ms /   256 runs   (    0.05 ms per token, 18514.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.43 ms /  1065 tokens (    4.04 ms per token,   247.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9521.79 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13994.32 ms /  1320 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.07 ms /   256 runs   (    0.05 ms per token, 18189.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.77 ms /  1039 tokens (    4.03 ms per token,   248.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9513.22 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13919.00 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18558.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4189.93 ms /  1038 tokens (    4.04 ms per token,   247.74 tokens per second)\n",
      "llama_print_timings:        eval time =    9470.62 ms /   255 runs   (   37.14 ms per token,    26.93 tokens per second)\n",
      "llama_print_timings:       total time =   13848.76 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1028 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18710.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4168.67 ms /  1028 tokens (    4.06 ms per token,   246.60 tokens per second)\n",
      "llama_print_timings:        eval time =    9487.57 ms /   255 runs   (   37.21 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13841.55 ms /  1283 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1063 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18854.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4300.01 ms /  1063 tokens (    4.05 ms per token,   247.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.75 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13996.88 ms /  1318 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 992 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18629.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3905.15 ms /   992 tokens (    3.94 ms per token,   254.02 tokens per second)\n",
      "llama_print_timings:        eval time =    9449.56 ms /   255 runs   (   37.06 ms per token,    26.99 tokens per second)\n",
      "llama_print_timings:       total time =   13563.13 ms /  1247 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1006 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /   256 runs   (    0.05 ms per token, 18415.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4034.61 ms /  1006 tokens (    4.01 ms per token,   249.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9433.28 ms /   255 runs   (   36.99 ms per token,    27.03 tokens per second)\n",
      "llama_print_timings:       total time =   13632.25 ms /  1261 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18637.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.55 ms /  1023 tokens (    3.95 ms per token,   253.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9460.15 ms /   255 runs   (   37.10 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13663.04 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18883.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4193.20 ms /  1047 tokens (    4.00 ms per token,   249.69 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.68 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13806.41 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.34 ms /   256 runs   (    0.06 ms per token, 17850.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4187.15 ms /  1052 tokens (    3.98 ms per token,   251.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.37 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13918.49 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1008 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   256 runs   (    0.05 ms per token, 18380.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4034.65 ms /  1008 tokens (    4.00 ms per token,   249.84 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.90 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13712.91 ms /  1263 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18754.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4188.11 ms /  1033 tokens (    4.05 ms per token,   246.65 tokens per second)\n",
      "llama_print_timings:        eval time =    9460.30 ms /   255 runs   (   37.10 ms per token,    26.95 tokens per second)\n",
      "llama_print_timings:       total time =   13798.51 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18667.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.57 ms /  1049 tokens (    3.99 ms per token,   250.68 tokens per second)\n",
      "llama_print_timings:        eval time =    9542.77 ms /   255 runs   (   37.42 ms per token,    26.72 tokens per second)\n",
      "llama_print_timings:       total time =   13960.37 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.02 ms /   256 runs   (    0.05 ms per token, 18259.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4206.95 ms /  1033 tokens (    4.07 ms per token,   245.55 tokens per second)\n",
      "llama_print_timings:        eval time =    9490.30 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13919.00 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.52 ms /   256 runs   (    0.05 ms per token, 18932.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4071.66 ms /  1020 tokens (    3.99 ms per token,   250.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.60 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13734.18 ms /  1275 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18697.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4205.90 ms /  1030 tokens (    4.08 ms per token,   244.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9474.38 ms /   255 runs   (   37.15 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13846.52 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.01 ms /   256 runs   (    0.05 ms per token, 18275.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.75 ms /  1040 tokens (    4.02 ms per token,   248.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9483.67 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13833.82 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1021 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /   256 runs   (    0.05 ms per token, 18606.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4054.68 ms /  1021 tokens (    3.97 ms per token,   251.81 tokens per second)\n",
      "llama_print_timings:        eval time =    9453.11 ms /   255 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =   13663.82 ms /  1276 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18724.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.58 ms /  1030 tokens (    4.06 ms per token,   246.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.61 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13854.49 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18717.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4191.99 ms /  1035 tokens (    4.05 ms per token,   246.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9486.79 ms /   255 runs   (   37.20 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13865.15 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.00 ms /   256 runs   (    0.05 ms per token, 18287.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.88 ms /  1047 tokens (    4.00 ms per token,   250.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9517.57 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13897.34 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 987 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.04 ms /   256 runs   (    0.05 ms per token, 18227.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3910.25 ms /   987 tokens (    3.96 ms per token,   252.41 tokens per second)\n",
      "llama_print_timings:        eval time =    9645.36 ms /   255 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
      "llama_print_timings:       total time =   13826.59 ms /  1242 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.82 ms /   256 runs   (    0.06 ms per token, 16178.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4509.98 ms /  1037 tokens (    4.35 ms per token,   229.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9615.17 ms /   255 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
      "llama_print_timings:       total time =   14345.20 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.83 ms /   256 runs   (    0.06 ms per token, 17265.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4219.70 ms /  1054 tokens (    4.00 ms per token,   249.78 tokens per second)\n",
      "llama_print_timings:        eval time =    9617.22 ms /   255 runs   (   37.71 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14031.59 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.58 ms /   256 runs   (    0.06 ms per token, 16436.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.46 ms /  1024 tokens (    3.95 ms per token,   253.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9584.82 ms /   255 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
      "llama_print_timings:       total time =   13818.42 ms /  1279 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1016 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.76 ms /   256 runs   (    0.06 ms per token, 16246.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4060.21 ms /  1016 tokens (    4.00 ms per token,   250.23 tokens per second)\n",
      "llama_print_timings:        eval time =    9631.92 ms /   255 runs   (   37.77 ms per token,    26.47 tokens per second)\n",
      "llama_print_timings:       total time =   13917.16 ms /  1271 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.35 ms /   256 runs   (    0.06 ms per token, 16676.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4054.03 ms /  1023 tokens (    3.96 ms per token,   252.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9617.08 ms /   255 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
      "llama_print_timings:       total time =   13872.20 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.51 ms /   256 runs   (    0.06 ms per token, 16503.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.53 ms /  1054 tokens (    3.98 ms per token,   251.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9649.56 ms /   255 runs   (   37.84 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14089.10 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.01 ms /   256 runs   (    0.06 ms per token, 15990.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.08 ms /  1036 tokens (    4.04 ms per token,   247.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9597.18 ms /   255 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
      "llama_print_timings:       total time =   13988.57 ms /  1291 tokens\n",
      "100%|██████████| 251/251 [58:06<00:00, 13.89s/it]    \n",
      "Extracting entities:   0%|          | 0/251 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/michieldekoninck/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/share/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/_static/nltk_cache'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m----> 6\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pipeline\u001b[39m(documents):\n\u001b[1;32m      5\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(transformations\u001b[38;5;241m=\u001b[39mtransformations)\n\u001b[0;32m----> 6\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39marun(documents\u001b[38;5;241m=\u001b[39mdocuments)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:297\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    290\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    291\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/ingestion/pipeline.py:727\u001b[0m, in \u001b[0;36mIngestionPipeline.arun\u001b[0;34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         nodes: Sequence[BaseNode] \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y, result, [])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m arun_transformations(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    728\u001b[0m         nodes_to_run,\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations,\n\u001b[1;32m    730\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    731\u001b[0m         cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    732\u001b[0m         cache_collection\u001b[38;5;241m=\u001b[39mcache_collection,\n\u001b[1;32m    733\u001b[0m         in_place\u001b[38;5;241m=\u001b[39min_place,\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[1;32m    738\u001b[0m nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/ingestion/pipeline.py:134\u001b[0m, in \u001b[0;36marun_transformations\u001b[0;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m cached_nodes\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transform\u001b[38;5;241m.\u001b[39macall(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    135\u001b[0m         cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/extractors/interface.py:170\u001b[0m, in \u001b[0;36mBaseExtractor.acall\u001b[0;34m(self, nodes, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macall\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post process nodes parsed from documents.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Allows extractors to be chained.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m        nodes (List[BaseNode]): nodes to post-process\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maprocess_nodes(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/extractors/interface.py:121\u001b[0m, in \u001b[0;36mBaseExtractor.aprocess_nodes\u001b[0;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     new_nodes \u001b[38;5;241m=\u001b[39m [deepcopy(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m--> 121\u001b[0m cur_metadata_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maextract(new_nodes)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_nodes):\n\u001b[1;32m    123\u001b[0m     node\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mupdate(cur_metadata_list[idx])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:297\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    290\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    291\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/extractors/entity/base.py:135\u001b[0m, in \u001b[0;36mEntityExtractor.aextract\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    133\u001b[0m metadata \u001b[38;5;241m=\u001b[39m metadata_list[i]\n\u001b[1;32m    134\u001b[0m node_text \u001b[38;5;241m=\u001b[39m nodes[i]\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_mode)\n\u001b[0;32m--> 135\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m spans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mpredict(words)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m spans:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/michieldekoninck/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/share/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/_static/nltk_cache'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "asyncio.run(run_pipeline(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": (\n",
    "                    node.metadata[\"document_title\"]\n",
    "                    + \"\\n\"\n",
    "                    + node.metadata[\"excerpt_keywords\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].text.index('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"symbol\": \"DNB\",\\n\"quarter\": 2,\\n\"year\": 2024,\\n\"date\": \"2024-08-03 13:51:09\",\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].text[:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=[Document.example()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
