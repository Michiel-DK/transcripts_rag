{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('json_data/transcripts_2024.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_ls = []\n",
    "\n",
    "for sub_ls in data[100:]:\n",
    "    for i in sub_ls:\n",
    "        new_data_ls.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5861"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(new_data_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_ls = ['AAPL', 'AMZN', 'GOOGL', 'MSFT', 'TSLA', 'CRSP', 'ABNB', 'ACN', 'ADYEY', 'ASML', 'CRM', 'EUXTF', 'FSLR', 'IBDSF', 'LDNXF', 'META', 'NVDA', 'SBGSF', 'SHOP', 'SPGI', 'TOITF', 'TTD', 'TTE', 'V'\\\n",
    "    'PATH', 'GCT', 'TMDX', 'NET', 'AXON', 'PL', 'OKTA', 'APP', 'CRWD', 'LULU', 'RKLB', 'NKE', 'AVGO', 'ZETA', 'UBER', 'SCMI', 'ISRG', 'TSM', 'ADSK', 'ARM', 'DDOG', 'HESAF', 'MDB', 'NOW', 'LVMHF', 'TWLO', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[df['symbol'].isin(st_ls)].drop_duplicates(subset=['symbol', 'date']).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_data_ls:\n",
    "    file_name = 'data/'+ str(i['year']) + '_' + str(i['quarter']) + '_' + i['symbol']+'.txt'\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(i['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transcripts_2024.json', 'w') as f:\n",
    "    json.dump(new_data_ls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.json import JSONReader\n",
    "\n",
    "# Initialize JSONReader\n",
    "reader = JSONReader(\n",
    "    # The number of levels to go back in the JSON tree. Set to 0 to traverse all levels. Default is None.\n",
    "    levels_back=0,\n",
    "    # The maximum number of characters a JSON fragment would be collapsed in the output. Default is None.\n",
    "    collapse_length=None,\n",
    "    # If True, ensures that the output is ASCII-encoded. Default is False.\n",
    "    ensure_ascii=True,\n",
    "    # If True, indicates that the file is in JSONL (JSON Lines) format. Default is False.\n",
    "    is_jsonl=True,\n",
    "    # If True, removes lines containing only formatting from the output. Default is True.\n",
    "    clean_json=True,\n",
    ")\n",
    "\n",
    "# Load data from JSON file\n",
    "documents = reader.load_data(input_file=\"transcripts_2024.json\", extra_info={})\n",
    "documents = documents[:6]\n",
    "#docs to nodesj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /Users/michieldekoninck/Library/Caches/llama_index/models/Meta-Llama-3-8B-Instruct.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = dl\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
      "llm_load_print_meta: general.name     = dl\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4156.02 MiB, ( 4156.08 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4156.00 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 7008\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   876.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  876.00 MiB, K (f16):  438.00 MiB, V (f16):  438.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   483.69 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.69 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'dl', 'llama.vocab_size': '128256'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from transcripts_rag.local_llama import load_model\n",
    "\n",
    "llm = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "# embed_model = CohereEmbeddings(model=\"embed-english-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.json import JSONReader\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "\n",
    "transformations = [\n",
    "    #JSONReader(levels_back=0, collapse_length=None,ensure_ascii=True,  is_jsonl=True,clean_json=True ),\n",
    "    SentenceSplitter(),\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    #QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    #SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    KeywordExtractor(keywords=5, llm=llm),\n",
    "    EntityExtractor(prediction_threshold=0.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('transcripts_2024.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "documents = documents[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "\n",
    "async def run_pipeline(documents):\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.90 ms /   256 runs   (    0.06 ms per token, 17182.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8578.71 ms /  1100 tokens (    7.80 ms per token,   128.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9341.07 ms /   255 runs   (   36.63 ms per token,    27.30 tokens per second)\n",
      "llama_print_timings:       total time =   18134.93 ms /  1355 tokens\n",
      "Llama.generate: 62 prefix-match hit, remaining 1010 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.88 ms /   256 runs   (    0.06 ms per token, 16116.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4049.28 ms /  1010 tokens (    4.01 ms per token,   249.43 tokens per second)\n",
      "llama_print_timings:        eval time =    9441.77 ms /   255 runs   (   37.03 ms per token,    27.01 tokens per second)\n",
      "llama_print_timings:       total time =   13783.32 ms /  1265 tokens\n",
      "Llama.generate: 61 prefix-match hit, remaining 1006 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.82 ms /   256 runs   (    0.06 ms per token, 16184.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.01 ms /  1006 tokens (    4.02 ms per token,   248.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.98 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13795.30 ms /  1261 tokens\n",
      "Llama.generate: 61 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.38 ms /   256 runs   (    0.06 ms per token, 16647.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4162.30 ms /  1044 tokens (    3.99 ms per token,   250.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9337.75 ms /   255 runs   (   36.62 ms per token,    27.31 tokens per second)\n",
      "llama_print_timings:       total time =   13726.21 ms /  1299 tokens\n",
      "Llama.generate: 61 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.73 ms /   256 runs   (    0.06 ms per token, 16274.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.77 ms /  1052 tokens (    3.98 ms per token,   251.27 tokens per second)\n",
      "llama_print_timings:        eval time =    9327.28 ms /   255 runs   (   36.58 ms per token,    27.34 tokens per second)\n",
      "llama_print_timings:       total time =   13798.13 ms /  1307 tokens\n",
      "100%|██████████| 5/5 [01:13<00:00, 14.68s/it]\n",
      "Llama.generate: 59 prefix-match hit, remaining 1306 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.09 ms /   256 runs   (    0.06 ms per token, 16962.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5144.69 ms /  1306 tokens (    3.94 ms per token,   253.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9629.52 ms /   255 runs   (   37.76 ms per token,    26.48 tokens per second)\n",
      "llama_print_timings:       total time =   14974.17 ms /  1561 tokens\n",
      "  0%|          | 0/251 [00:00<?, ?it/s]Llama.generate: 60 prefix-match hit, remaining 1326 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.49 ms /   256 runs   (    0.06 ms per token, 16531.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5314.47 ms /  1326 tokens (    4.01 ms per token,   249.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9670.39 ms /   255 runs   (   37.92 ms per token,    26.37 tokens per second)\n",
      "llama_print_timings:       total time =   15210.89 ms /  1581 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1048 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.64 ms /   256 runs   (    0.06 ms per token, 16365.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4240.49 ms /  1048 tokens (    4.05 ms per token,   247.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9639.52 ms /   255 runs   (   37.80 ms per token,    26.45 tokens per second)\n",
      "llama_print_timings:       total time =   14127.29 ms /  1303 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 993 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.13 ms /   256 runs   (    0.06 ms per token, 16924.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4045.05 ms /   993 tokens (    4.07 ms per token,   245.49 tokens per second)\n",
      "llama_print_timings:        eval time =    9590.23 ms /   255 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
      "llama_print_timings:       total time =   13920.12 ms /  1248 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.36 ms /   256 runs   (    0.06 ms per token, 15647.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.86 ms /  1040 tokens (    4.05 ms per token,   247.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9631.93 ms /   255 runs   (   37.77 ms per token,    26.47 tokens per second)\n",
      "llama_print_timings:       total time =   14091.46 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1055 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.79 ms /   256 runs   (    0.06 ms per token, 16217.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4228.45 ms /  1055 tokens (    4.01 ms per token,   249.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9644.48 ms /   255 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
      "llama_print_timings:       total time =   14084.18 ms /  1310 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.80 ms /   256 runs   (    0.06 ms per token, 17301.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4207.42 ms /  1037 tokens (    4.06 ms per token,   246.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9605.35 ms /   255 runs   (   37.67 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =   14037.54 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 994 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.80 ms /   256 runs   (    0.06 ms per token, 16199.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.68 ms /   994 tokens (    4.07 ms per token,   245.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9590.43 ms /   255 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
      "llama_print_timings:       total time =   13933.28 ms /  1249 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /   256 runs   (    0.06 ms per token, 17106.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.84 ms /  1043 tokens (    4.04 ms per token,   247.64 tokens per second)\n",
      "llama_print_timings:        eval time =    9638.57 ms /   255 runs   (   37.80 ms per token,    26.46 tokens per second)\n",
      "llama_print_timings:       total time =   14094.38 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.33 ms /   256 runs   (    0.06 ms per token, 16702.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4228.98 ms /  1054 tokens (    4.01 ms per token,   249.23 tokens per second)\n",
      "llama_print_timings:        eval time =    9619.46 ms /   255 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14062.13 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.51 ms /   256 runs   (    0.06 ms per token, 16501.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4204.59 ms /  1050 tokens (    4.00 ms per token,   249.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9619.97 ms /   255 runs   (   37.73 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14048.34 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1028 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.42 ms /   256 runs   (    0.06 ms per token, 16598.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4191.78 ms /  1028 tokens (    4.08 ms per token,   245.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9594.42 ms /   255 runs   (   37.63 ms per token,    26.58 tokens per second)\n",
      "llama_print_timings:       total time =   14063.64 ms /  1283 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.20 ms /   256 runs   (    0.06 ms per token, 16838.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.04 ms /  1050 tokens (    4.01 ms per token,   249.11 tokens per second)\n",
      "llama_print_timings:        eval time =    9642.03 ms /   255 runs   (   37.81 ms per token,    26.45 tokens per second)\n",
      "llama_print_timings:       total time =   14097.80 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.93 ms /   256 runs   (    0.06 ms per token, 16068.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4201.10 ms /  1047 tokens (    4.01 ms per token,   249.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9649.80 ms /   255 runs   (   37.84 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14117.05 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.64 ms /   256 runs   (    0.07 ms per token, 15383.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4196.82 ms /  1046 tokens (    4.01 ms per token,   249.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9703.17 ms /   255 runs   (   38.05 ms per token,    26.28 tokens per second)\n",
      "llama_print_timings:       total time =   14192.32 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.02 ms /   256 runs   (    0.06 ms per token, 15985.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4214.00 ms /  1037 tokens (    4.06 ms per token,   246.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9619.06 ms /   255 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14076.80 ms /  1292 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1048 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.68 ms /   256 runs   (    0.06 ms per token, 16323.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.23 ms /  1048 tokens (    4.02 ms per token,   249.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9636.75 ms /   255 runs   (   37.79 ms per token,    26.46 tokens per second)\n",
      "llama_print_timings:       total time =   14113.44 ms /  1303 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /   256 runs   (    0.06 ms per token, 17110.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4223.72 ms /  1038 tokens (    4.07 ms per token,   245.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9604.46 ms /   255 runs   (   37.66 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =   14069.52 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1059 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.83 ms /   256 runs   (    0.06 ms per token, 16168.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4320.97 ms /  1059 tokens (    4.08 ms per token,   245.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9677.61 ms /   255 runs   (   37.95 ms per token,    26.35 tokens per second)\n",
      "llama_print_timings:       total time =   14227.88 ms /  1314 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.79 ms /   256 runs   (    0.06 ms per token, 16209.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4205.39 ms /  1033 tokens (    4.07 ms per token,   245.64 tokens per second)\n",
      "llama_print_timings:        eval time =    9617.88 ms /   255 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14079.89 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.59 ms /   256 runs   (    0.06 ms per token, 16417.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.16 ms /  1051 tokens (    4.01 ms per token,   249.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9645.74 ms /   255 runs   (   37.83 ms per token,    26.44 tokens per second)\n",
      "llama_print_timings:       total time =   14080.07 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.04 ms /   256 runs   (    0.06 ms per token, 15958.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4223.35 ms /  1040 tokens (    4.06 ms per token,   246.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9624.21 ms /   255 runs   (   37.74 ms per token,    26.50 tokens per second)\n",
      "llama_print_timings:       total time =   14106.64 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.29 ms /   256 runs   (    0.06 ms per token, 15719.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4218.89 ms /  1053 tokens (    4.01 ms per token,   249.59 tokens per second)\n",
      "llama_print_timings:        eval time =    9649.52 ms /   255 runs   (   37.84 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14156.66 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.82 ms /   256 runs   (    0.06 ms per token, 16181.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4209.15 ms /  1027 tokens (    4.10 ms per token,   243.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9610.03 ms /   255 runs   (   37.69 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14059.25 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      17.20 ms /   256 runs   (    0.07 ms per token, 14885.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4199.78 ms /  1029 tokens (    4.08 ms per token,   245.01 tokens per second)\n",
      "llama_print_timings:        eval time =    9647.64 ms /   255 runs   (   37.83 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14166.13 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1003 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.81 ms /   256 runs   (    0.06 ms per token, 16187.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4123.43 ms /  1003 tokens (    4.11 ms per token,   243.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9598.99 ms /   255 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
      "llama_print_timings:       total time =   13963.34 ms /  1258 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.61 ms /   256 runs   (    0.06 ms per token, 16403.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4219.21 ms /  1049 tokens (    4.02 ms per token,   248.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9635.88 ms /   255 runs   (   37.79 ms per token,    26.46 tokens per second)\n",
      "llama_print_timings:       total time =   14067.57 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.05 ms /   256 runs   (    0.06 ms per token, 17009.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4202.47 ms /  1026 tokens (    4.10 ms per token,   244.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9609.35 ms /   255 runs   (   37.68 ms per token,    26.54 tokens per second)\n",
      "llama_print_timings:       total time =   14042.84 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.87 ms /   256 runs   (    0.06 ms per token, 16127.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1245.90 ms /   312 tokens (    3.99 ms per token,   250.42 tokens per second)\n",
      "llama_print_timings:        eval time =    8711.62 ms /   255 runs   (   34.16 ms per token,    29.27 tokens per second)\n",
      "llama_print_timings:       total time =   10170.45 ms /   567 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1032 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.57 ms /   256 runs   (    0.06 ms per token, 16443.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4212.70 ms /  1032 tokens (    4.08 ms per token,   244.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9611.21 ms /   255 runs   (   37.69 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14085.01 ms /  1287 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.91 ms /   256 runs   (    0.06 ms per token, 17171.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4230.76 ms /  1040 tokens (    4.07 ms per token,   245.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9623.80 ms /   255 runs   (   37.74 ms per token,    26.50 tokens per second)\n",
      "llama_print_timings:       total time =   14089.06 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.58 ms /   256 runs   (    0.06 ms per token, 16433.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.77 ms /  1049 tokens (    4.02 ms per token,   248.83 tokens per second)\n",
      "llama_print_timings:        eval time =    9622.48 ms /   255 runs   (   37.74 ms per token,    26.50 tokens per second)\n",
      "llama_print_timings:       total time =   14052.94 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.97 ms /   256 runs   (    0.06 ms per token, 16033.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4215.77 ms /  1042 tokens (    4.05 ms per token,   247.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9626.33 ms /   255 runs   (   37.75 ms per token,    26.49 tokens per second)\n",
      "llama_print_timings:       total time =   14072.39 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.77 ms /   256 runs   (    0.06 ms per token, 16230.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4206.04 ms /  1051 tokens (    4.00 ms per token,   249.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9630.83 ms /   255 runs   (   37.77 ms per token,    26.48 tokens per second)\n",
      "llama_print_timings:       total time =   14086.99 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1002 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.63 ms /   256 runs   (    0.06 ms per token, 16380.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.58 ms /  1002 tokens (    4.03 ms per token,   248.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9595.72 ms /   255 runs   (   37.63 ms per token,    26.57 tokens per second)\n",
      "llama_print_timings:       total time =   13895.80 ms /  1257 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.39 ms /   256 runs   (    0.06 ms per token, 16636.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4201.66 ms /  1038 tokens (    4.05 ms per token,   247.05 tokens per second)\n",
      "llama_print_timings:        eval time =    9605.23 ms /   255 runs   (   37.67 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =   14012.50 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.22 ms /   256 runs   (    0.06 ms per token, 15786.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4193.40 ms /  1035 tokens (    4.05 ms per token,   246.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9612.50 ms /   255 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14045.39 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.21 ms /   256 runs   (    0.06 ms per token, 16836.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4207.52 ms /  1027 tokens (    4.10 ms per token,   244.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9625.81 ms /   255 runs   (   37.75 ms per token,    26.49 tokens per second)\n",
      "llama_print_timings:       total time =   14140.21 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 988 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.75 ms /   256 runs   (    0.06 ms per token, 16253.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3947.08 ms /   988 tokens (    4.00 ms per token,   250.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9569.63 ms /   255 runs   (   37.53 ms per token,    26.65 tokens per second)\n",
      "llama_print_timings:       total time =   13758.23 ms /  1243 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.87 ms /   256 runs   (    0.06 ms per token, 16134.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.96 ms /  1045 tokens (    4.03 ms per token,   248.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9613.30 ms /   255 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   14043.51 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.00 ms /   256 runs   (    0.06 ms per token, 17065.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.03 ms /  1053 tokens (    4.00 ms per token,   250.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9633.85 ms /   255 runs   (   37.78 ms per token,    26.47 tokens per second)\n",
      "llama_print_timings:       total time =   14063.77 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.08 ms /   256 runs   (    0.06 ms per token, 15918.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4222.60 ms /  1030 tokens (    4.10 ms per token,   243.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9531.78 ms /   255 runs   (   37.38 ms per token,    26.75 tokens per second)\n",
      "llama_print_timings:       total time =   14002.21 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18721.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.19 ms /  1042 tokens (    4.02 ms per token,   248.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9507.64 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13883.58 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.81 ms /   256 runs   (    0.05 ms per token, 18534.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.97 ms /  1051 tokens (    3.99 ms per token,   250.54 tokens per second)\n",
      "llama_print_timings:        eval time =    9497.11 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13857.25 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.77 ms /   256 runs   (    0.05 ms per token, 18595.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4171.67 ms /  1030 tokens (    4.05 ms per token,   246.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9477.84 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13848.98 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.77 ms /   256 runs   (    0.05 ms per token, 18591.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.25 ms /  1038 tokens (    4.03 ms per token,   248.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9480.64 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13828.51 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18304.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4175.18 ms /  1038 tokens (    4.02 ms per token,   248.61 tokens per second)\n",
      "llama_print_timings:        eval time =    9517.44 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13930.99 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /   256 runs   (    0.05 ms per token, 18905.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.33 ms /  1050 tokens (    3.99 ms per token,   250.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.22 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13893.31 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.25 ms /   256 runs   (    0.06 ms per token, 17963.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4189.58 ms /  1046 tokens (    4.01 ms per token,   249.67 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.95 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13895.59 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1032 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.09 ms /   256 runs   (    0.06 ms per token, 18163.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.96 ms /  1032 tokens (    4.05 ms per token,   247.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9566.37 ms /   255 runs   (   37.52 ms per token,    26.66 tokens per second)\n",
      "llama_print_timings:       total time =   14087.47 ms /  1287 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18797.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.31 ms /  1042 tokens (    4.04 ms per token,   247.43 tokens per second)\n",
      "llama_print_timings:        eval time =    9493.66 ms /   255 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13932.25 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1065 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.85 ms /   256 runs   (    0.05 ms per token, 18490.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.97 ms /  1065 tokens (    4.04 ms per token,   247.68 tokens per second)\n",
      "llama_print_timings:        eval time =    9557.42 ms /   255 runs   (   37.48 ms per token,    26.68 tokens per second)\n",
      "llama_print_timings:       total time =   14046.63 ms /  1320 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1071 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /   256 runs   (    0.05 ms per token, 18898.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4303.55 ms /  1071 tokens (    4.02 ms per token,   248.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9520.38 ms /   255 runs   (   37.33 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13993.72 ms /  1326 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18294.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4043.94 ms /  1022 tokens (    3.96 ms per token,   252.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9457.07 ms /   255 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13672.90 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1060 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18668.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4296.16 ms /  1060 tokens (    4.05 ms per token,   246.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9538.01 ms /   255 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   14032.58 ms /  1315 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18466.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.16 ms /  1041 tokens (    4.03 ms per token,   248.20 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.64 ms /   255 runs   (   37.29 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13898.29 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.35 ms /   256 runs   (    0.06 ms per token, 17837.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.06 ms /  1029 tokens (    4.06 ms per token,   246.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9524.30 ms /   255 runs   (   37.35 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13962.29 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1067 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18798.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4333.76 ms /  1067 tokens (    4.06 ms per token,   246.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9535.50 ms /   255 runs   (   37.39 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   14060.17 ms /  1322 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18855.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.61 ms /  1050 tokens (    3.99 ms per token,   250.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.99 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13876.28 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18721.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.88 ms /  1039 tokens (    4.03 ms per token,   248.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.91 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13902.09 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /   256 runs   (    0.05 ms per token, 18422.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4304.19 ms /  1066 tokens (    4.04 ms per token,   247.67 tokens per second)\n",
      "llama_print_timings:        eval time =    9526.96 ms /   255 runs   (   37.36 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   14030.23 ms /  1321 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.78 ms /   256 runs   (    0.05 ms per token, 18581.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.70 ms /  1036 tokens (    4.04 ms per token,   247.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9466.69 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13813.64 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1002 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18652.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4027.42 ms /  1002 tokens (    4.02 ms per token,   248.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9429.43 ms /   255 runs   (   36.98 ms per token,    27.04 tokens per second)\n",
      "llama_print_timings:       total time =   13634.19 ms /  1257 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18475.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.55 ms /  1053 tokens (    3.97 ms per token,   251.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9526.77 ms /   255 runs   (   37.36 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13911.53 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18388.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.90 ms /  1041 tokens (    4.02 ms per token,   248.63 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.55 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13868.29 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.94 ms /   256 runs   (    0.05 ms per token, 18368.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.95 ms /  1039 tokens (    4.02 ms per token,   248.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.07 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13889.61 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18669.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4303.60 ms /  1066 tokens (    4.04 ms per token,   247.70 tokens per second)\n",
      "llama_print_timings:        eval time =    9515.39 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   14007.42 ms /  1321 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1057 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18566.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4305.86 ms /  1057 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9521.62 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   14017.90 ms /  1312 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /   256 runs   (    0.05 ms per token, 18607.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.49 ms /  1037 tokens (    4.04 ms per token,   247.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.27 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13915.70 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.60 ms /   256 runs   (    0.05 ms per token, 18820.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.26 ms /  1034 tokens (    4.04 ms per token,   247.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.71 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13823.62 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1056 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.49 ms /   256 runs   (    0.05 ms per token, 18979.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.28 ms /  1056 tokens (    3.96 ms per token,   252.37 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.10 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13870.20 ms /  1311 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.03 ms /   256 runs   (    0.05 ms per token, 18247.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.60 ms /  1039 tokens (    4.02 ms per token,   248.59 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.06 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13844.77 ms /  1294 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18961.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.06 ms /  1023 tokens (    3.94 ms per token,   253.78 tokens per second)\n",
      "llama_print_timings:        eval time =    9464.35 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13680.38 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.39 ms /   256 runs   (    0.05 ms per token, 19117.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.03 ms /  1037 tokens (    4.03 ms per token,   247.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9459.85 ms /   255 runs   (   37.10 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13794.85 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18870.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.51 ms /  1040 tokens (    4.02 ms per token,   248.95 tokens per second)\n",
      "llama_print_timings:        eval time =    9495.66 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13861.12 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18867.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4038.04 ms /  1022 tokens (    3.95 ms per token,   253.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9449.48 ms /   255 runs   (   37.06 ms per token,    26.99 tokens per second)\n",
      "llama_print_timings:       total time =   13653.48 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.19 ms /   256 runs   (    0.06 ms per token, 18043.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.03 ms /  1050 tokens (    3.98 ms per token,   251.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9515.69 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13954.71 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18860.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.61 ms /  1040 tokens (    4.02 ms per token,   248.65 tokens per second)\n",
      "llama_print_timings:        eval time =    9508.75 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13880.52 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1061 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18615.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4303.98 ms /  1061 tokens (    4.06 ms per token,   246.52 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.35 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   14000.69 ms /  1316 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.03 ms /   256 runs   (    0.05 ms per token, 18241.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.89 ms /  1043 tokens (    4.01 ms per token,   249.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9503.28 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13877.56 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.96 ms /   256 runs   (    0.05 ms per token, 18342.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.92 ms /  1034 tokens (    4.04 ms per token,   247.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.03 ms /   255 runs   (   37.21 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13865.56 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18815.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.51 ms /  1039 tokens (    4.03 ms per token,   248.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.99 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13870.14 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18996.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4032.80 ms /  1020 tokens (    3.95 ms per token,   252.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9461.57 ms /   255 runs   (   37.10 ms per token,    26.95 tokens per second)\n",
      "llama_print_timings:       total time =   13681.01 ms /  1275 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /   256 runs   (    0.05 ms per token, 18782.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.12 ms /  1038 tokens (    4.03 ms per token,   248.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9472.66 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13815.14 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1031 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18958.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.91 ms /  1031 tokens (    4.05 ms per token,   246.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9476.11 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13831.28 ms /  1286 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1056 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.89 ms /   256 runs   (    0.05 ms per token, 18425.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.31 ms /  1056 tokens (    3.95 ms per token,   253.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9507.57 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13872.87 ms /  1311 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18831.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.48 ms /  1026 tokens (    4.08 ms per token,   244.84 tokens per second)\n",
      "llama_print_timings:        eval time =    9474.63 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13852.50 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18809.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.71 ms /  1049 tokens (    3.99 ms per token,   250.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9490.84 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13844.24 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1062 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.52 ms /   256 runs   (    0.05 ms per token, 18936.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4302.71 ms /  1062 tokens (    4.05 ms per token,   246.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9513.82 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   14002.07 ms /  1317 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1005 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.11 ms /   256 runs   (    0.06 ms per token, 18141.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4027.08 ms /  1005 tokens (    4.01 ms per token,   249.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.66 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13699.11 ms /  1260 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /   256 runs   (    0.05 ms per token, 18913.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.15 ms /  1044 tokens (    4.01 ms per token,   249.39 tokens per second)\n",
      "llama_print_timings:        eval time =    9495.74 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13857.88 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.87 ms /   256 runs   (    0.05 ms per token, 18455.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.50 ms /  1029 tokens (    4.06 ms per token,   246.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.43 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13854.83 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1011 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18883.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.66 ms /  1011 tokens (    3.99 ms per token,   250.77 tokens per second)\n",
      "llama_print_timings:        eval time =    9444.07 ms /   255 runs   (   37.04 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13653.04 ms /  1266 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18394.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.22 ms /  1054 tokens (    3.97 ms per token,   251.96 tokens per second)\n",
      "llama_print_timings:        eval time =    9524.68 ms /   255 runs   (   37.35 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13932.81 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.40 ms /   256 runs   (    0.06 ms per token, 17779.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4172.15 ms /  1030 tokens (    4.05 ms per token,   246.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9510.66 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13907.13 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18852.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.99 ms /  1043 tokens (    4.02 ms per token,   248.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.21 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13861.88 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1053 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18986.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.44 ms /  1053 tokens (    3.97 ms per token,   251.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.15 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13830.01 ms /  1308 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18699.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.80 ms /  1040 tokens (    4.03 ms per token,   248.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9484.62 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13840.03 ms /  1295 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18962.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.20 ms /  1044 tokens (    4.00 ms per token,   249.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9497.56 ms /   255 runs   (   37.25 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13880.18 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1055 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /   256 runs   (    0.05 ms per token, 18897.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.97 ms /  1055 tokens (    3.97 ms per token,   252.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9495.55 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13843.26 ms /  1310 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18215.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4258.06 ms /  1047 tokens (    4.07 ms per token,   245.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.09 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13938.63 ms /  1302 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /   256 runs   (    0.05 ms per token, 18606.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.79 ms /  1036 tokens (    4.03 ms per token,   247.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9478.26 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13828.76 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1067 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.64 ms /   256 runs   (    0.05 ms per token, 18761.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4298.54 ms /  1067 tokens (    4.03 ms per token,   248.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9548.35 ms /   255 runs   (   37.44 ms per token,    26.71 tokens per second)\n",
      "llama_print_timings:       total time =   14051.85 ms /  1322 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1048 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /   256 runs   (    0.05 ms per token, 18784.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.23 ms /  1048 tokens (    3.99 ms per token,   250.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9522.61 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13912.09 ms /  1303 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.73 ms /   256 runs   (    0.05 ms per token, 18646.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.69 ms /  1035 tokens (    4.03 ms per token,   247.92 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.34 ms /   255 runs   (   37.22 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13836.46 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1070 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18791.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4304.82 ms /  1070 tokens (    4.02 ms per token,   248.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9511.97 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13983.81 ms /  1325 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1060 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.94 ms /   256 runs   (    0.05 ms per token, 18360.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4300.52 ms /  1060 tokens (    4.06 ms per token,   246.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9547.33 ms /   255 runs   (   37.44 ms per token,    26.71 tokens per second)\n",
      "llama_print_timings:       total time =   14082.08 ms /  1315 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.24 ms /   256 runs   (    0.06 ms per token, 17977.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.83 ms /  1026 tokens (    4.07 ms per token,   245.64 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.77 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13888.66 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18392.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4188.26 ms /  1051 tokens (    3.99 ms per token,   250.94 tokens per second)\n",
      "llama_print_timings:        eval time =    9517.79 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13906.50 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.77 ms /   256 runs   (    0.05 ms per token, 18593.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.45 ms /  1046 tokens (    4.00 ms per token,   249.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.94 ms /   255 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13851.84 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.94 ms /   256 runs   (    0.05 ms per token, 18367.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.90 ms /  1020 tokens (    3.96 ms per token,   252.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9464.58 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13676.00 ms /  1275 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18809.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.25 ms /  1049 tokens (    4.00 ms per token,   250.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.51 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13909.58 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   256 runs   (    0.05 ms per token, 18958.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4227.80 ms /  1050 tokens (    4.03 ms per token,   248.36 tokens per second)\n",
      "llama_print_timings:        eval time =    9507.55 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13923.04 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18548.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.35 ms /  1042 tokens (    4.02 ms per token,   248.55 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.48 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13927.28 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1056 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18754.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.68 ms /  1056 tokens (    3.96 ms per token,   252.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.50 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13843.22 ms /  1311 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      17.45 ms /   256 runs   (    0.07 ms per token, 14671.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4057.51 ms /  1022 tokens (    3.97 ms per token,   251.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9477.18 ms /   255 runs   (   37.17 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13752.94 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1002 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.61 ms /   256 runs   (    0.05 ms per token, 18816.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4035.59 ms /  1002 tokens (    4.03 ms per token,   248.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9445.03 ms /   255 runs   (   37.04 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13662.22 ms /  1257 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.87 ms /   256 runs   (    0.05 ms per token, 18458.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.13 ms /  1045 tokens (    4.00 ms per token,   249.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.99 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13851.02 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1010 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18471.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.45 ms /  1010 tokens (    4.00 ms per token,   249.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9443.73 ms /   255 runs   (   37.03 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13672.71 ms /  1265 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18717.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.10 ms /  1039 tokens (    4.02 ms per token,   248.80 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.36 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13841.39 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18631.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.18 ms /  1042 tokens (    4.01 ms per token,   249.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9506.72 ms /   255 runs   (   37.28 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13889.75 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18742.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.60 ms /  1034 tokens (    4.04 ms per token,   247.33 tokens per second)\n",
      "llama_print_timings:        eval time =    9466.57 ms /   255 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
      "llama_print_timings:       total time =   13818.19 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18223.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.62 ms /  1046 tokens (    4.00 ms per token,   250.02 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.70 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13898.89 ms /  1301 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.60 ms /   256 runs   (    0.05 ms per token, 18829.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.39 ms /  1047 tokens (    4.00 ms per token,   250.28 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.14 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13865.10 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18656.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.43 ms /  1026 tokens (    4.08 ms per token,   245.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9502.52 ms /   255 runs   (   37.26 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13899.72 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18865.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4074.73 ms /  1023 tokens (    3.98 ms per token,   251.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9463.66 ms /   255 runs   (   37.11 ms per token,    26.95 tokens per second)\n",
      "llama_print_timings:       total time =   13724.91 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18223.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4310.48 ms /  1058 tokens (    4.07 ms per token,   245.45 tokens per second)\n",
      "llama_print_timings:        eval time =    9520.74 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   14009.72 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 944 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18701.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3778.02 ms /   944 tokens (    4.00 ms per token,   249.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9360.10 ms /   255 runs   (   36.71 ms per token,    27.24 tokens per second)\n",
      "llama_print_timings:       total time =   13337.99 ms /  1199 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18662.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.69 ms /  1036 tokens (    4.04 ms per token,   247.57 tokens per second)\n",
      "llama_print_timings:        eval time =    9518.45 ms /   255 runs   (   37.33 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13905.94 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.95 ms /   256 runs   (    0.05 ms per token, 18352.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.53 ms /  1037 tokens (    4.03 ms per token,   247.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9502.77 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13868.09 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1003 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.11 ms /   256 runs   (    0.06 ms per token, 18149.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4021.79 ms /  1003 tokens (    4.01 ms per token,   249.39 tokens per second)\n",
      "llama_print_timings:        eval time =    9456.87 ms /   255 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13671.59 ms /  1258 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 962 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.09 ms /   256 runs   (    0.06 ms per token, 18163.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3897.89 ms /   962 tokens (    4.05 ms per token,   246.80 tokens per second)\n",
      "llama_print_timings:        eval time =    9403.56 ms /   255 runs   (   36.88 ms per token,    27.12 tokens per second)\n",
      "llama_print_timings:       total time =   13491.64 ms /  1217 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1073 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.12 ms /   256 runs   (    0.06 ms per token, 18132.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4333.98 ms /  1073 tokens (    4.04 ms per token,   247.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.45 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   14084.75 ms /  1328 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.00 ms /   256 runs   (    0.05 ms per token, 18287.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.92 ms /  1030 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.28 ms /   255 runs   (   37.21 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13865.39 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1008 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.52 ms /   256 runs   (    0.05 ms per token, 18930.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4036.51 ms /  1008 tokens (    4.00 ms per token,   249.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9432.53 ms /   255 runs   (   36.99 ms per token,    27.03 tokens per second)\n",
      "llama_print_timings:       total time =   13617.70 ms /  1263 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1065 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18754.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4307.26 ms /  1065 tokens (    4.04 ms per token,   247.26 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.78 ms /   255 runs   (   37.31 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13979.03 ms /  1320 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18869.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.86 ms /  1052 tokens (    3.98 ms per token,   251.26 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.18 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13809.04 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1026 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /   256 runs   (    0.05 ms per token, 18414.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.79 ms /  1026 tokens (    4.07 ms per token,   245.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.58 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13819.57 ms /  1281 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1064 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18849.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4316.49 ms /  1064 tokens (    4.06 ms per token,   246.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9504.36 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13983.58 ms /  1319 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.46 ms /   256 runs   (    0.05 ms per token, 19017.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.22 ms /  1042 tokens (    4.03 ms per token,   248.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9478.49 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13824.29 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.57 ms /   256 runs   (    0.05 ms per token, 18859.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.91 ms /  1035 tokens (    4.03 ms per token,   247.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.36 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13769.29 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1067 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.84 ms /   256 runs   (    0.05 ms per token, 18502.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4297.97 ms /  1067 tokens (    4.03 ms per token,   248.26 tokens per second)\n",
      "llama_print_timings:        eval time =    9537.65 ms /   255 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   14042.94 ms /  1322 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1022 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18790.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4046.55 ms /  1022 tokens (    3.96 ms per token,   252.56 tokens per second)\n",
      "llama_print_timings:        eval time =    9472.50 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13697.90 ms /  1277 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.94 ms /   256 runs   (    0.06 ms per token, 17137.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.09 ms /  1058 tokens (    4.06 ms per token,   246.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9589.28 ms /   255 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
      "llama_print_timings:       total time =   14082.27 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1071 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.37 ms /   256 runs   (    0.06 ms per token, 17819.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4302.12 ms /  1071 tokens (    4.02 ms per token,   248.95 tokens per second)\n",
      "llama_print_timings:        eval time =    9534.20 ms /   255 runs   (   37.39 ms per token,    26.75 tokens per second)\n",
      "llama_print_timings:       total time =   13991.12 ms /  1326 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1007 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.73 ms /   256 runs   (    0.05 ms per token, 18645.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4024.59 ms /  1007 tokens (    4.00 ms per token,   250.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.49 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13669.50 ms /  1262 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.82 ms /   256 runs   (    0.05 ms per token, 18526.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4211.45 ms /  1050 tokens (    4.01 ms per token,   249.32 tokens per second)\n",
      "llama_print_timings:        eval time =    9525.19 ms /   255 runs   (   37.35 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time =   13933.99 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1025 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18995.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.18 ms /  1024 tokens (    3.94 ms per token,   254.02 tokens per second)\n",
      "llama_print_timings:        eval time =    9477.58 ms /   256 runs   (   37.02 ms per token,    27.01 tokens per second)\n",
      "llama_print_timings:       total time =   13658.83 ms /  1280 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18467.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4173.88 ms /  1044 tokens (    4.00 ms per token,   250.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.38 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13794.70 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18717.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.92 ms /  1036 tokens (    4.03 ms per token,   247.91 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.26 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13772.56 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18615.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.60 ms /  1037 tokens (    4.03 ms per token,   248.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9490.05 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13860.65 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1008 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.45 ms /   256 runs   (    0.05 ms per token, 19029.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4027.90 ms /  1008 tokens (    4.00 ms per token,   250.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9418.03 ms /   255 runs   (   36.93 ms per token,    27.08 tokens per second)\n",
      "llama_print_timings:       total time =   13618.01 ms /  1263 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.05 ms /   256 runs   (    0.05 ms per token, 18215.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4180.65 ms /  1041 tokens (    4.02 ms per token,   249.00 tokens per second)\n",
      "llama_print_timings:        eval time =    9528.49 ms /   255 runs   (   37.37 ms per token,    26.76 tokens per second)\n",
      "llama_print_timings:       total time =   13898.87 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18548.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.82 ms /  1039 tokens (    4.03 ms per token,   248.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.94 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13837.49 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1009 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18566.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4034.88 ms /  1009 tokens (    4.00 ms per token,   250.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9446.98 ms /   255 runs   (   37.05 ms per token,    26.99 tokens per second)\n",
      "llama_print_timings:       total time =   13665.70 ms /  1264 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1059 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18612.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.80 ms /  1059 tokens (    4.06 ms per token,   246.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.72 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13986.85 ms /  1314 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /   256 runs   (    0.05 ms per token, 18790.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.29 ms /  1033 tokens (    4.04 ms per token,   247.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.89 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13841.00 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18841.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.80 ms /  1038 tokens (    4.02 ms per token,   248.52 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.50 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13813.06 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1004 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.49 ms /   256 runs   (    0.05 ms per token, 18981.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4024.85 ms /  1004 tokens (    4.01 ms per token,   249.45 tokens per second)\n",
      "llama_print_timings:        eval time =    9423.35 ms /   255 runs   (   36.95 ms per token,    27.06 tokens per second)\n",
      "llama_print_timings:       total time =   13593.61 ms /  1259 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   256 runs   (    0.05 ms per token, 18380.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4023.37 ms /  1024 tokens (    3.93 ms per token,   254.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.67 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13701.04 ms /  1279 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.89 ms /   256 runs   (    0.05 ms per token, 18431.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.13 ms /  1037 tokens (    4.04 ms per token,   247.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.15 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13843.00 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.51 ms /   256 runs   (    0.05 ms per token, 18955.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.09 ms /  1035 tokens (    4.04 ms per token,   247.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9474.11 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13826.35 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.46 ms /   256 runs   (    0.05 ms per token, 19013.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.51 ms /  1041 tokens (    4.03 ms per token,   248.12 tokens per second)\n",
      "llama_print_timings:        eval time =    9478.15 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13839.97 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.88 ms /   256 runs   (    0.05 ms per token, 18445.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4193.85 ms /  1036 tokens (    4.05 ms per token,   247.03 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.90 ms /   255 runs   (   37.31 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13913.10 ms /  1291 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18736.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.15 ms /  1052 tokens (    4.00 ms per token,   249.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.70 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13923.82 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1018 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18838.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4045.95 ms /  1018 tokens (    3.97 ms per token,   251.61 tokens per second)\n",
      "llama_print_timings:        eval time =    9454.32 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13671.92 ms /  1273 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /   256 runs   (    0.05 ms per token, 18783.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.23 ms /  1045 tokens (    4.01 ms per token,   249.09 tokens per second)\n",
      "llama_print_timings:        eval time =    9504.88 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13897.75 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18561.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.70 ms /  1029 tokens (    4.08 ms per token,   245.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9508.20 ms /   255 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
      "llama_print_timings:       total time =   13918.22 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.84 ms /   256 runs   (    0.05 ms per token, 18494.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.78 ms /  1042 tokens (    4.02 ms per token,   249.00 tokens per second)\n",
      "llama_print_timings:        eval time =    9510.91 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13894.36 ms /  1297 tokens\n",
      "Llama.generate: 335 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /   256 runs   (    0.05 ms per token, 18842.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.88 ms /  1027 tokens (    4.07 ms per token,   245.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9479.55 ms /   255 runs   (   37.17 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13839.43 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.43 ms /   256 runs   (    0.05 ms per token, 19063.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4300.33 ms /  1058 tokens (    4.06 ms per token,   246.03 tokens per second)\n",
      "llama_print_timings:        eval time =    9506.06 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   14003.66 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1019 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.96 ms /   256 runs   (    0.05 ms per token, 18343.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.81 ms /  1019 tokens (    3.97 ms per token,   251.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9480.84 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13713.04 ms /  1274 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18881.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4176.41 ms /  1029 tokens (    4.06 ms per token,   246.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9503.43 ms /   255 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13882.64 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1029 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.49 ms /   256 runs   (    0.05 ms per token, 18974.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.87 ms /  1029 tokens (    4.07 ms per token,   245.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.87 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13831.29 ms /  1284 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18552.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4186.70 ms /  1047 tokens (    4.00 ms per token,   250.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9496.10 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13869.99 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1012 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18623.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4041.14 ms /  1012 tokens (    3.99 ms per token,   250.42 tokens per second)\n",
      "llama_print_timings:        eval time =    9443.59 ms /   255 runs   (   37.03 ms per token,    27.00 tokens per second)\n",
      "llama_print_timings:       total time =   13650.55 ms /  1267 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1027 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18738.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.53 ms /  1027 tokens (    4.07 ms per token,   245.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.21 ms /   255 runs   (   37.22 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13868.78 ms /  1282 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1011 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18733.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4041.01 ms /  1011 tokens (    4.00 ms per token,   250.18 tokens per second)\n",
      "llama_print_timings:        eval time =    9442.51 ms /   255 runs   (   37.03 ms per token,    27.01 tokens per second)\n",
      "llama_print_timings:       total time =   13662.37 ms /  1266 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1050 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18469.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.25 ms /  1050 tokens (    3.98 ms per token,   251.36 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.09 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13877.49 ms /  1305 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1018 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18673.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.48 ms /  1018 tokens (    3.97 ms per token,   252.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9457.72 ms /   255 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13691.53 ms /  1273 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1017 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.87 ms /   256 runs   (    0.06 ms per token, 17214.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.40 ms /  1017 tokens (    3.98 ms per token,   251.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.60 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13704.97 ms /  1272 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18852.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.93 ms /  1038 tokens (    4.03 ms per token,   248.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9497.21 ms /   255 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13854.91 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1055 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18626.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.58 ms /  1055 tokens (    3.97 ms per token,   251.63 tokens per second)\n",
      "llama_print_timings:        eval time =    9515.20 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13911.59 ms /  1310 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18725.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.38 ms /  1023 tokens (    3.95 ms per token,   253.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9451.91 ms /   255 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =   13661.55 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1019 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18849.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4050.01 ms /  1019 tokens (    3.97 ms per token,   251.60 tokens per second)\n",
      "llama_print_timings:        eval time =    9469.38 ms /   255 runs   (   37.13 ms per token,    26.93 tokens per second)\n",
      "llama_print_timings:       total time =   13691.09 ms /  1274 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.01 ms /   256 runs   (    0.05 ms per token, 18271.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4179.08 ms /  1044 tokens (    4.00 ms per token,   249.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9519.64 ms /   255 runs   (   37.33 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13902.39 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1057 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.83 ms /   256 runs   (    0.05 ms per token, 18517.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4296.85 ms /  1057 tokens (    4.07 ms per token,   245.99 tokens per second)\n",
      "llama_print_timings:        eval time =    9527.49 ms /   255 runs   (   37.36 ms per token,    26.76 tokens per second)\n",
      "llama_print_timings:       total time =   14025.51 ms /  1312 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18705.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.45 ms /  1030 tokens (    4.05 ms per token,   246.74 tokens per second)\n",
      "llama_print_timings:        eval time =    9476.96 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13841.80 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18293.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.26 ms /  1034 tokens (    4.04 ms per token,   247.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.96 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13835.74 ms /  1289 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   256 runs   (    0.05 ms per token, 18374.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4175.75 ms /  1040 tokens (    4.02 ms per token,   249.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9523.53 ms /   255 runs   (   37.35 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13911.86 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1025 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18716.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4046.55 ms /  1024 tokens (    3.95 ms per token,   253.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9509.28 ms /   256 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13759.56 ms /  1280 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1041 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.54 ms /   256 runs   (    0.05 ms per token, 18912.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4189.79 ms /  1041 tokens (    4.02 ms per token,   248.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9487.16 ms /   255 runs   (   37.20 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13848.39 ms /  1296 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1031 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18703.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4187.34 ms /  1031 tokens (    4.06 ms per token,   246.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9483.52 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13856.72 ms /  1286 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1015 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      20.11 ms /   256 runs   (    0.08 ms per token, 12729.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.11 ms /  1015 tokens (    3.98 ms per token,   251.42 tokens per second)\n",
      "llama_print_timings:        eval time =    9489.68 ms /   255 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13717.04 ms /  1270 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.51 ms /   256 runs   (    0.05 ms per token, 18943.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4196.94 ms /  1040 tokens (    4.04 ms per token,   247.80 tokens per second)\n",
      "llama_print_timings:        eval time =    9492.43 ms /   255 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =   13885.87 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1051 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18697.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.16 ms /  1051 tokens (    3.98 ms per token,   251.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9518.86 ms /   255 runs   (   37.33 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13894.06 ms /  1306 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1031 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.31 ms /   256 runs   (    0.06 ms per token, 17884.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4171.19 ms /  1031 tokens (    4.05 ms per token,   247.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.92 ms /   255 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13853.01 ms /  1286 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 999 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.01 ms /   256 runs   (    0.05 ms per token, 18275.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4021.12 ms /   999 tokens (    4.03 ms per token,   248.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9475.60 ms /   255 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13670.74 ms /  1254 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18664.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4187.50 ms /  1042 tokens (    4.02 ms per token,   248.84 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.21 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13901.06 ms /  1297 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1014 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   256 runs   (    0.05 ms per token, 18995.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4070.63 ms /  1014 tokens (    4.01 ms per token,   249.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9454.83 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13712.56 ms /  1269 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.99 ms /   256 runs   (    0.05 ms per token, 18301.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.97 ms /  1052 tokens (    3.99 ms per token,   250.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9499.26 ms /   255 runs   (   37.25 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13864.26 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1013 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.73 ms /   256 runs   (    0.05 ms per token, 18638.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4037.87 ms /  1013 tokens (    3.99 ms per token,   250.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9451.69 ms /   255 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =   13674.93 ms /  1268 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18471.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.24 ms /  1047 tokens (    4.00 ms per token,   250.28 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.30 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13893.85 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1021 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   256 runs   (    0.05 ms per token, 18389.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4065.02 ms /  1021 tokens (    3.98 ms per token,   251.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9455.79 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13699.19 ms /  1276 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1032 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.66 ms /   256 runs   (    0.05 ms per token, 18747.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4174.85 ms /  1032 tokens (    4.05 ms per token,   247.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9498.64 ms /   255 runs   (   37.25 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13874.72 ms /  1287 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.41 ms /   256 runs   (    0.05 ms per token, 19093.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4182.83 ms /  1045 tokens (    4.00 ms per token,   249.83 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.20 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13835.86 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.86 ms /   256 runs   (    0.05 ms per token, 18470.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.22 ms /  1054 tokens (    3.97 ms per token,   251.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9537.48 ms /   255 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
      "llama_print_timings:       total time =   13940.54 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1018 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.00 ms /   256 runs   (    0.05 ms per token, 18283.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4038.38 ms /  1018 tokens (    3.97 ms per token,   252.08 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.95 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13762.00 ms /  1273 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.90 ms /   256 runs   (    0.06 ms per token, 16100.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.57 ms /  1045 tokens (    4.00 ms per token,   249.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9488.38 ms /   255 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13872.15 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 993 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.14 ms /   256 runs   (    0.06 ms per token, 18100.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4015.37 ms /   993 tokens (    4.04 ms per token,   247.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9454.61 ms /   255 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
      "llama_print_timings:       total time =   13662.68 ms /  1248 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.97 ms /   256 runs   (    0.05 ms per token, 18322.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4031.28 ms /  1024 tokens (    3.94 ms per token,   254.01 tokens per second)\n",
      "llama_print_timings:        eval time =    9481.15 ms /   255 runs   (   37.18 ms per token,    26.90 tokens per second)\n",
      "llama_print_timings:       total time =   13687.84 ms /  1279 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.72 ms /   256 runs   (    0.05 ms per token, 18658.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4199.18 ms /  1039 tokens (    4.04 ms per token,   247.43 tokens per second)\n",
      "llama_print_timings:        eval time =    9528.39 ms /   255 runs   (   37.37 ms per token,    26.76 tokens per second)\n",
      "llama_print_timings:       total time =   13937.29 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   256 runs   (    0.05 ms per token, 18618.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.29 ms /  1037 tokens (    4.03 ms per token,   248.01 tokens per second)\n",
      "llama_print_timings:        eval time =    9483.17 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13851.92 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1017 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18545.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4033.99 ms /  1017 tokens (    3.97 ms per token,   252.11 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.46 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13690.37 ms /  1272 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.78 ms /   256 runs   (    0.05 ms per token, 18574.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.58 ms /  1052 tokens (    3.97 ms per token,   251.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.62 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13836.47 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1044 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.53 ms /   256 runs   (    0.05 ms per token, 18918.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.63 ms /  1044 tokens (    4.01 ms per token,   249.48 tokens per second)\n",
      "llama_print_timings:        eval time =    9498.19 ms /   255 runs   (   37.25 ms per token,    26.85 tokens per second)\n",
      "llama_print_timings:       total time =   13875.96 ms /  1299 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18705.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.07 ms /  1039 tokens (    4.03 ms per token,   248.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9470.74 ms /   255 runs   (   37.14 ms per token,    26.93 tokens per second)\n",
      "llama_print_timings:       total time =   13830.10 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.06 ms /   256 runs   (    0.05 ms per token, 18206.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4177.55 ms /  1043 tokens (    4.01 ms per token,   249.67 tokens per second)\n",
      "llama_print_timings:        eval time =    9505.52 ms /   255 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =   13852.25 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.84 ms /   256 runs   (    0.05 ms per token, 18501.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4305.60 ms /  1058 tokens (    4.07 ms per token,   245.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9500.22 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13963.39 ms /  1313 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1069 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18753.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4309.29 ms /  1069 tokens (    4.03 ms per token,   248.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9533.36 ms /   255 runs   (   37.39 ms per token,    26.75 tokens per second)\n",
      "llama_print_timings:       total time =   14007.90 ms /  1324 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18876.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4192.27 ms /  1043 tokens (    4.02 ms per token,   248.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9491.32 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13860.13 ms /  1298 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.55 ms /   256 runs   (    0.06 ms per token, 17588.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.19 ms /  1040 tokens (    4.02 ms per token,   248.55 tokens per second)\n",
      "llama_print_timings:        eval time =    9501.97 ms /   255 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =   13888.33 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1045 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /   256 runs   (    0.05 ms per token, 18550.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.50 ms /  1045 tokens (    4.01 ms per token,   249.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9514.93 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13923.99 ms /  1300 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1065 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.83 ms /   256 runs   (    0.05 ms per token, 18514.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4299.43 ms /  1065 tokens (    4.04 ms per token,   247.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9521.79 ms /   255 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =   13994.32 ms /  1320 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1039 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.07 ms /   256 runs   (    0.05 ms per token, 18189.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4183.77 ms /  1039 tokens (    4.03 ms per token,   248.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9513.22 ms /   255 runs   (   37.31 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13919.00 ms /  1294 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1038 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   256 runs   (    0.05 ms per token, 18558.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4189.93 ms /  1038 tokens (    4.04 ms per token,   247.74 tokens per second)\n",
      "llama_print_timings:        eval time =    9470.62 ms /   255 runs   (   37.14 ms per token,    26.93 tokens per second)\n",
      "llama_print_timings:       total time =   13848.76 ms /  1293 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1028 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18710.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4168.67 ms /  1028 tokens (    4.06 ms per token,   246.60 tokens per second)\n",
      "llama_print_timings:        eval time =    9487.57 ms /   255 runs   (   37.21 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13841.55 ms /  1283 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1063 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.58 ms /   256 runs   (    0.05 ms per token, 18854.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4300.01 ms /  1063 tokens (    4.05 ms per token,   247.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.75 ms /   255 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
      "llama_print_timings:       total time =   13996.88 ms /  1318 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 992 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18629.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3905.15 ms /   992 tokens (    3.94 ms per token,   254.02 tokens per second)\n",
      "llama_print_timings:        eval time =    9449.56 ms /   255 runs   (   37.06 ms per token,    26.99 tokens per second)\n",
      "llama_print_timings:       total time =   13563.13 ms /  1247 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1006 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.90 ms /   256 runs   (    0.05 ms per token, 18415.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4034.61 ms /  1006 tokens (    4.01 ms per token,   249.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9433.28 ms /   255 runs   (   36.99 ms per token,    27.03 tokens per second)\n",
      "llama_print_timings:       total time =   13632.25 ms /  1261 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /   256 runs   (    0.05 ms per token, 18637.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.55 ms /  1023 tokens (    3.95 ms per token,   253.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9460.15 ms /   255 runs   (   37.10 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =   13663.04 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.56 ms /   256 runs   (    0.05 ms per token, 18883.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4193.20 ms /  1047 tokens (    4.00 ms per token,   249.69 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.68 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13806.41 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1052 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.34 ms /   256 runs   (    0.06 ms per token, 17850.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4187.15 ms /  1052 tokens (    3.98 ms per token,   251.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9516.37 ms /   255 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
      "llama_print_timings:       total time =   13918.49 ms /  1307 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1008 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   256 runs   (    0.05 ms per token, 18380.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4034.65 ms /  1008 tokens (    4.00 ms per token,   249.84 tokens per second)\n",
      "llama_print_timings:        eval time =    9471.90 ms /   255 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13712.91 ms /  1263 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.65 ms /   256 runs   (    0.05 ms per token, 18754.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4188.11 ms /  1033 tokens (    4.05 ms per token,   246.65 tokens per second)\n",
      "llama_print_timings:        eval time =    9460.30 ms /   255 runs   (   37.10 ms per token,    26.95 tokens per second)\n",
      "llama_print_timings:       total time =   13798.51 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.71 ms /   256 runs   (    0.05 ms per token, 18667.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.57 ms /  1049 tokens (    3.99 ms per token,   250.68 tokens per second)\n",
      "llama_print_timings:        eval time =    9542.77 ms /   255 runs   (   37.42 ms per token,    26.72 tokens per second)\n",
      "llama_print_timings:       total time =   13960.37 ms /  1304 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1033 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.02 ms /   256 runs   (    0.05 ms per token, 18259.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4206.95 ms /  1033 tokens (    4.07 ms per token,   245.55 tokens per second)\n",
      "llama_print_timings:        eval time =    9490.30 ms /   255 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =   13919.00 ms /  1288 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.52 ms /   256 runs   (    0.05 ms per token, 18932.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4071.66 ms /  1020 tokens (    3.99 ms per token,   250.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9473.60 ms /   255 runs   (   37.15 ms per token,    26.92 tokens per second)\n",
      "llama_print_timings:       total time =   13734.18 ms /  1275 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.69 ms /   256 runs   (    0.05 ms per token, 18697.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4205.90 ms /  1030 tokens (    4.08 ms per token,   244.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9474.38 ms /   255 runs   (   37.15 ms per token,    26.91 tokens per second)\n",
      "llama_print_timings:       total time =   13846.52 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1040 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.01 ms /   256 runs   (    0.05 ms per token, 18275.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4185.75 ms /  1040 tokens (    4.02 ms per token,   248.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9483.67 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13833.82 ms /  1295 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1021 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /   256 runs   (    0.05 ms per token, 18606.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4054.68 ms /  1021 tokens (    3.97 ms per token,   251.81 tokens per second)\n",
      "llama_print_timings:        eval time =    9453.11 ms /   255 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =   13663.82 ms /  1276 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.67 ms /   256 runs   (    0.05 ms per token, 18724.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4178.58 ms /  1030 tokens (    4.06 ms per token,   246.50 tokens per second)\n",
      "llama_print_timings:        eval time =    9482.61 ms /   255 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =   13854.49 ms /  1285 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1035 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      13.68 ms /   256 runs   (    0.05 ms per token, 18717.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4191.99 ms /  1035 tokens (    4.05 ms per token,   246.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9486.79 ms /   255 runs   (   37.20 ms per token,    26.88 tokens per second)\n",
      "llama_print_timings:       total time =   13865.15 ms /  1290 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1047 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.00 ms /   256 runs   (    0.05 ms per token, 18287.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4184.88 ms /  1047 tokens (    4.00 ms per token,   250.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9517.57 ms /   255 runs   (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_print_timings:       total time =   13897.34 ms /  1302 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 987 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.04 ms /   256 runs   (    0.05 ms per token, 18227.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3910.25 ms /   987 tokens (    3.96 ms per token,   252.41 tokens per second)\n",
      "llama_print_timings:        eval time =    9645.36 ms /   255 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
      "llama_print_timings:       total time =   13826.59 ms /  1242 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1037 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.82 ms /   256 runs   (    0.06 ms per token, 16178.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4509.98 ms /  1037 tokens (    4.35 ms per token,   229.93 tokens per second)\n",
      "llama_print_timings:        eval time =    9615.17 ms /   255 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
      "llama_print_timings:       total time =   14345.20 ms /  1292 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      14.83 ms /   256 runs   (    0.06 ms per token, 17265.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4219.70 ms /  1054 tokens (    4.00 ms per token,   249.78 tokens per second)\n",
      "llama_print_timings:        eval time =    9617.22 ms /   255 runs   (   37.71 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   14031.59 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.58 ms /   256 runs   (    0.06 ms per token, 16436.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4044.46 ms /  1024 tokens (    3.95 ms per token,   253.19 tokens per second)\n",
      "llama_print_timings:        eval time =    9584.82 ms /   255 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
      "llama_print_timings:       total time =   13818.42 ms /  1279 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1016 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.76 ms /   256 runs   (    0.06 ms per token, 16246.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4060.21 ms /  1016 tokens (    4.00 ms per token,   250.23 tokens per second)\n",
      "llama_print_timings:        eval time =    9631.92 ms /   255 runs   (   37.77 ms per token,    26.47 tokens per second)\n",
      "llama_print_timings:       total time =   13917.16 ms /  1271 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1023 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.35 ms /   256 runs   (    0.06 ms per token, 16676.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4054.03 ms /  1023 tokens (    3.96 ms per token,   252.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9617.08 ms /   255 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
      "llama_print_timings:       total time =   13872.20 ms /  1278 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1054 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      15.51 ms /   256 runs   (    0.06 ms per token, 16503.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.53 ms /  1054 tokens (    3.98 ms per token,   251.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9649.56 ms /   255 runs   (   37.84 ms per token,    26.43 tokens per second)\n",
      "llama_print_timings:       total time =   14089.10 ms /  1309 tokens\n",
      "Llama.generate: 334 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    6124.42 ms\n",
      "llama_print_timings:      sample time =      16.01 ms /   256 runs   (    0.06 ms per token, 15990.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.08 ms /  1036 tokens (    4.04 ms per token,   247.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9597.18 ms /   255 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
      "llama_print_timings:       total time =   13988.57 ms /  1291 tokens\n",
      "100%|██████████| 251/251 [58:06<00:00, 13.89s/it]    \n",
      "Extracting entities:   0%|          | 0/251 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/michieldekoninck/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/share/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/_static/nltk_cache'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m----> 6\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pipeline\u001b[39m(documents):\n\u001b[1;32m      5\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(transformations\u001b[38;5;241m=\u001b[39mtransformations)\n\u001b[0;32m----> 6\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39marun(documents\u001b[38;5;241m=\u001b[39mdocuments)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:297\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    290\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    291\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/ingestion/pipeline.py:727\u001b[0m, in \u001b[0;36mIngestionPipeline.arun\u001b[0;34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         nodes: Sequence[BaseNode] \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y, result, [])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m arun_transformations(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    728\u001b[0m         nodes_to_run,\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations,\n\u001b[1;32m    730\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    731\u001b[0m         cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    732\u001b[0m         cache_collection\u001b[38;5;241m=\u001b[39mcache_collection,\n\u001b[1;32m    733\u001b[0m         in_place\u001b[38;5;241m=\u001b[39min_place,\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[1;32m    738\u001b[0m nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/ingestion/pipeline.py:134\u001b[0m, in \u001b[0;36marun_transformations\u001b[0;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m cached_nodes\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transform\u001b[38;5;241m.\u001b[39macall(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    135\u001b[0m         cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/extractors/interface.py:170\u001b[0m, in \u001b[0;36mBaseExtractor.acall\u001b[0;34m(self, nodes, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macall\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post process nodes parsed from documents.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Allows extractors to be chained.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m        nodes (List[BaseNode]): nodes to post-process\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maprocess_nodes(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/extractors/interface.py:121\u001b[0m, in \u001b[0;36mBaseExtractor.aprocess_nodes\u001b[0;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     new_nodes \u001b[38;5;241m=\u001b[39m [deepcopy(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m--> 121\u001b[0m cur_metadata_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maextract(new_nodes)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_nodes):\n\u001b[1;32m    123\u001b[0m     node\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mupdate(cur_metadata_list[idx])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:297\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    290\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    291\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/extractors/entity/base.py:135\u001b[0m, in \u001b[0;36mEntityExtractor.aextract\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    133\u001b[0m metadata \u001b[38;5;241m=\u001b[39m metadata_list[i]\n\u001b[1;32m    134\u001b[0m node_text \u001b[38;5;241m=\u001b[39m nodes[i]\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_mode)\n\u001b[0;32m--> 135\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m spans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mpredict(words)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m spans:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/michieldekoninck/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/share/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/transcripts_rag/lib/python3.10/site-packages/llama_index/core/_static/nltk_cache'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "asyncio.run(run_pipeline(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": (\n",
    "                    node.metadata[\"document_title\"]\n",
    "                    + \"\\n\"\n",
    "                    + node.metadata[\"excerpt_keywords\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].text.index('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"symbol\": \"DNB\",\\n\"quarter\": 2,\\n\"year\": 2024,\\n\"date\": \"2024-08-03 13:51:09\",\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].text[:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=[Document.example()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
